<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="emmeans package, Version 1.10.5" />


<title>Sophisticated models in emmeans</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {font-size: 11pt; font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;margin: 30px 50px 30px 50px; }h1,h2,h3,h4,h5,h6 { font-family: Arial,Helvetica,Sans-serif; }a { text-decoration: none; }a:link { color:darkblue; } a:visited { color:darkblue; } a:hover { color:dodgerblue; }a:active { color:dodgerblue; } code {color: #602000;font-family: "Lucida Console", Monaco, monospace; font-size: 90%;}.r { color: darkred; }.ro { color: darkgreen; background-color: #eeeeee; }.re { color: red;}.r code, a code, .ro code, .re code { color: inherit; }.vigindex ul { list-style-type: none; }.vigindex ul li { list-style: none; }.vigindex a code { color: inherit; }.vigindex li code { color: inherit; }</style>




</head>

<body>




<h1 class="title toc-ignore">Sophisticated models in emmeans</h1>
<h4 class="author">emmeans package, Version 1.10.5</h4>



<!-- @index Vignettes!Sophisticated models -->
<p>This vignette gives a few examples of the use of the
<strong>emmeans</strong> package to analyze other than the basic types
of models provided by the <strong>stats</strong> package. Emphasis here
is placed on accessing the optional capabilities that are typically not
needed for the more basic models. A reference for all supported models
is provided in the <a href="models.html">“models” vignette</a>.</p>
<div id="contents" class="section level2">
<h2>Contents</h2>
<ol style="list-style-type: decimal">
<li><a href="#lmer">Linear mixed models (lmer)</a>
<ol style="list-style-type: lower-alpha">
<li><a href="#lmerOpts">System options for lmerMod models</a></li>
<li><a href="#random-slopes">Bias adjustment with random slopes</a></li>
</ol></li>
<li><a href="#offsets">Models with offsets</a></li>
<li><a href="#ordinal">Ordinal models</a></li>
<li><a href="#mcmc">Models fitted using MCMC methods</a></li>
</ol>
<p><a href="vignette-topics.html">Index of all vignette topics</a></p>
</div>
<div id="lmer" class="section level2">
<h2>Linear mixed models (lmer)</h2>
<!-- @index `lmerMod` models; Examples!`Oats`; Examples!Split-plot experiment -->
<p>Linear mixed models are really important in statistics. Emphasis here
is placed on those fitted using <code>lme4::lmer()</code>, but
<strong>emmeans</strong> also supports other mixed-model packages such
as <strong>nlme</strong>.</p>
<p>To illustrate, consider the <code>Oats</code> dataset in the
<strong>nlme</strong> package. It has the results of a balanced
split-plot experiment: experimental blocks are divided into plots that
are randomly assigned to oat varieties, and the plots are subdivided
into subplots that are randomly assigned to amounts of nitrogen within
each plot. We will consider a linear mixed model for these data,
excluding interaction (which is justified in this case). For sake of
illustration, we will exclude a few observations.</p>
<pre class="r"><code>library(lme4)
Oats.lmer &lt;- lmer(yield ~ Variety + factor(nitro) + (1|Block/Variety),
                        data = nlme::Oats, subset = -c(1,2,3,5,8,13,21,34,55))</code></pre>
<p>Let’s look at the EMMs for <code>nitro</code>:</p>
<pre class="r"><code>Oats.emm.n &lt;- emmeans(Oats.lmer, &quot;nitro&quot;)
Oats.emm.n</code></pre>
<pre class="ro"><code>##  nitro emmean   SE   df lower.CL upper.CL
##    0.0   78.9 7.29 7.78     62.0     95.8
##    0.2   97.0 7.14 7.19     80.3    113.8
##    0.4  114.2 7.14 7.19     97.4    131.0
##    0.6  124.1 7.07 6.95    107.3    140.8
## 
## Results are averaged over the levels of: Variety 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95</code></pre>
<p>You will notice that the degrees of freedom are fractional: that is
due to the fact that whole-plot and subplot variations are combined when
standard errors are estimated. Different degrees-of-freedom methods are
available. By default, the Kenward-Roger method is used, and that’s why
you see a message about the <strong>pbkrtest</strong> package being
loaded, as it implements that method. We may specify a different
degrees-of-freedom method via the optional argument
<code>lmer.df</code>:</p>
<pre class="r"><code>emmeans(Oats.lmer, &quot;nitro&quot;, lmer.df = &quot;satterthwaite&quot;)</code></pre>
<pre class="ro"><code>##  nitro emmean   SE   df lower.CL upper.CL
##    0.0   78.9 7.28 7.28     61.8       96
##    0.2   97.0 7.13 6.72     80.0      114
##    0.4  114.2 7.13 6.72     97.2      131
##    0.6  124.1 7.07 6.49    107.1      141
## 
## Results are averaged over the levels of: Variety 
## Degrees-of-freedom method: satterthwaite 
## Confidence level used: 0.95</code></pre>
<div id="dfoptions" class="section level6">
<h6></h6>
<!-- @index Degrees of freedom; *z* tests; Asymptotic tests -->
<p>This latest result uses the Satterthwaite method, which is
implemented in the <strong>lmerTest</strong> package. Note that, with
this method, not only are the degrees of freedom slightly different, but
so are the standard errors. That is because the Kenward-Roger method
also entails making a bias adjustment to the covariance matrix of the
fixed effects; that is the principal difference between the methods. A
third possibility is <code>&quot;asymptotic&quot;</code>:</p>
<pre class="r"><code>emmeans(Oats.lmer, &quot;nitro&quot;, lmer.df = &quot;asymptotic&quot;)</code></pre>
<pre class="ro"><code>##  nitro emmean   SE  df asymp.LCL asymp.UCL
##    0.0   78.9 7.28 Inf      64.6      93.2
##    0.2   97.0 7.13 Inf      83.1     111.0
##    0.4  114.2 7.13 Inf     100.2     128.2
##    0.6  124.1 7.07 Inf     110.2     137.9
## 
## Results are averaged over the levels of: Variety 
## Degrees-of-freedom method: asymptotic 
## Confidence level used: 0.95</code></pre>
<p>This just sets all the degrees of freedom to <code>Inf</code> –
that’s <strong>emmeans</strong>’s way of using <em>z</em> statistics
rather than <em>t</em> statistics. The asymptotic methods tend to make
confidence intervals a bit too narrow and P values a bit too low; but
they involve much, much less computation. Note that the SEs are the same
as obtained using the Satterthwaite method.</p>
<p>Comparisons and contrasts are pretty much the same as with other
models. As <code>nitro</code> has quantitative levels, we might want to
test polynomial contrasts:</p>
<pre class="r"><code>contrast(Oats.emm.n, &quot;poly&quot;)</code></pre>
<pre class="ro"><code>##  contrast  estimate    SE   df t.ratio p.value
##  linear      152.69 15.60 43.2   9.802  &lt;.0001
##  quadratic    -8.27  6.95 44.2  -1.190  0.2402
##  cubic        -6.32 15.20 42.8  -0.415  0.6800
## 
## Results are averaged over the levels of: Variety 
## Degrees-of-freedom method: kenward-roger</code></pre>
<p>The interesting thing here is that the degrees of freedom are much
larger than they are for the EMMs. The reason is because
<code>nitro</code> within-plot factor, so inter-plot variations have
little role in estimating contrasts among <code>nitro</code> levels. On
the other hand, <code>Variety</code> is a whole-plot factor, and there
is not much of a bump in degrees of freedom for comparisons:</p>
<pre class="r"><code>emmeans(Oats.lmer, pairwise ~ Variety)</code></pre>
<pre class="ro"><code>## $emmeans
##  Variety     emmean   SE   df lower.CL upper.CL
##  Golden Rain  105.2 7.53 8.46     88.0      122
##  Marvellous   108.5 7.48 8.28     91.3      126
##  Victory       96.9 7.64 8.81     79.6      114
## 
## Results are averaged over the levels of: nitro 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95 
## 
## $contrasts
##  contrast                 estimate   SE   df t.ratio p.value
##  Golden Rain - Marvellous    -3.23 6.55 9.56  -0.493  0.8764
##  Golden Rain - Victory        8.31 6.71 9.80   1.238  0.4595
##  Marvellous - Victory        11.54 6.67 9.80   1.729  0.2431
## 
## Results are averaged over the levels of: nitro 
## Degrees-of-freedom method: kenward-roger 
## P value adjustment: tukey method for comparing a family of 3 estimates</code></pre>
</div>
<div id="lmerOpts" class="section level3">
<h3>System options for lmerMod models</h3>
<!-- @index `lmerMod` models!System options for -->
<p>The computation required to compute the adjusted covariance matrix
and degrees of freedom may become cumbersome. Some user options (i.e.,
<code>emm_options()</code> calls) make it possible to streamline these
computations through default methods and limitations on them. First, the
option <code>lmer.df</code>, which may have values of
<code>&quot;kenward-roger&quot;</code>, <code>&quot;satterthwaite&quot;</code>, or
<code>&quot;asymptotic&quot;</code> (partial matches are OK!) specifies the
default degrees-of-freedom method.</p>
<p>The options <code>disable.pbkrtest</code> and
<code>disable.lmerTest</code> may be <code>TRUE</code> or
<code>FALSE</code>, and comprise another way of controlling which method
is used (e.g., the Kenward-Roger method will not be used if
<code>get_emm_option(&quot;disable.pbkrtest&quot;) == TRUE</code>). Finally, the
options <code>pbkrtest.limit</code> and <code>lmerTest.limit</code>,
which should be set to numeric values, enable the given package
conditionally on whether the number of data rows does not exceed the
given limit. The factory default is 3000 for both limits.</p>
</div>
<div id="random-slopes" class="section level3">
<h3>Bias adjustment with random slopes</h3>
<!-- @index Bias adjustment!with random slopes; Random slopes!Bias adjustment;
    Examples!`ChickWeight` -->
<p>In the <a href="transformations.html#cbpp"><code>cbpp</code>
example</a>, we saw an example where we applied a bias adjustment to the
inverse transformation. To do that adjustment, we required an estimate
of the total SD of the response. That computation was (relatively)
simple because the model had only random intercepts. But what if we have
random slopes as well? The short answer is “it gets more complicated;”
but here is an example of how we can muddle through it.</p>
<p>Our illustration is a model fitted to the <code>ChickWeight</code>
data in the R <strong>datasets</strong> package. The data comprise
weight determinations, over time (in days since birth), of chicks who
are randomized to different diets. Our model fits a trend in the square
root of weight for each diet, with random intercepts and slopes for each
chick (this is likely not the best model, but it’s not totally stupid
and it serves an an illustration).</p>
<pre class="r"><code>cw.lmer &lt;- lmer(sqrt(weight) ~ Time*Diet + (1+Time|Chick), data = ChickWeight)</code></pre>
<p>Our goal is to use this model to estimate the mean
<code>weight</code> at times 5, 10, 15, and 20, for each diet.
Accordingly, let’s get the estimates needed:</p>
<pre class="r"><code>cw.emm &lt;- emmeans(cw.lmer, ~ Time|Diet, at = list(Time = c(5, 10, 15, 20)))</code></pre>
<p>If we just summarize this with `type = “response”, we will
under-estimate the mean weights. We need to apply a bias adjustment; but
that involves providing an estimate of the SD of each transformed
response. The problem is that since random slopes are involved, that SD
depends on time. In particular, the model states that at time <span class="math inline">\(t\)</span>, <span class="math display">\[ \sqrt
Y_t = \mu_t + E + C + S\times t \]</span> where <span class="math inline">\(\mu_t\)</span> is the mean at time <span class="math inline">\(t\)</span>, <span class="math inline">\(E\)</span>
is the residual error, <span class="math inline">\(C\)</span> is the
random intercept for chicks, and <span class="math inline">\(S\)</span>
is the random slope for chicks. For purposes of bias correction, we need
an estimate of <span class="math inline">\(SD(E + C + S\times
t)\)</span> for each <span class="math inline">\(t\)</span>.</p>
<p>The first step is to obtain an estimated covariance matrix for <span class="math inline">\((E, C, S)\)</span>:</p>
<pre class="r"><code>V &lt;- matrix(0, nrow = 3, ncol = 3, dimnames = list(c(&quot;E&quot;,&quot;C&quot;,&quot;S&quot;), c(&quot;E&quot;,&quot;C&quot;,&quot;S&quot;)))
V[1, 1] &lt;- sigma(cw.lmer)^2              # Var(E)
V[2:3, 2:3] &lt;- VarCorr(cw.lmer)$Chick    # Cov(C, S)
V</code></pre>
<pre class="ro"><code>##           E           C           S
## E 0.1867732  0.00000000  0.00000000
## C 0.0000000  0.15918129 -0.03977984
## S 0.0000000 -0.03977984  0.01513793</code></pre>
<p>Now, using the matrix expression <span class="math inline">\(Var(a&#39;X) = a&#39;Va\)</span> for <span class="math inline">\(X=c(E,C,S)\)</span> and a given vector <span class="math inline">\(a\)</span>, we can obtain the needed SDs:</p>
<pre class="r"><code>sig &lt;- sapply(c(5, 10, 15, 20), function(t) {
    a &lt;- c(1, 1, t)
    sqrt(sum(a * V %*% a))
})
sig  </code></pre>
<pre class="ro"><code>## [1] 0.5714931 1.0315769 1.5995606 2.1931561</code></pre>
<p>As expected, these values increase with <span class="math inline">\(t\)</span>. Finally, we obtain the bias-adjusted
estimated weights. We can use <code>sigma = sig</code> as-is since the
values follow the same ordering as <code>cw.emm@grid</code>.</p>
<pre class="r"><code>confint(cw.emm, type = &quot;response&quot;, bias.adj = TRUE, sigma = sig)</code></pre>
<pre class="ro"><code>## Diet = 1:
##  Time response    SE   df lower.CL upper.CL
##     5     63.2  1.49 45.5     60.2     66.2
##    10     91.1  4.11 45.8     83.0     99.5
##    15    124.5  7.81 46.1    109.3    140.8
##    20    163.6 12.40 46.2    139.6    189.7
## 
## Diet = 2:
##  Time response    SE   df lower.CL upper.CL
##     5     69.3  2.15 45.0     65.1     73.7
##    10    106.6  6.14 44.9     94.6    119.3
##    15    152.3 12.00 44.9    129.1    177.4
##    20    206.5 19.40 44.9    169.3    247.5
## 
## Diet = 3:
##  Time response    SE   df lower.CL upper.CL
##     5     72.8  2.20 45.0     68.4     77.3
##    10    121.3  6.56 44.9    108.5    134.9
##    15    182.7 13.10 44.9    157.2    210.1
##    20    256.9 21.70 44.9    215.1    302.4
## 
## Diet = 4:
##  Time response    SE   df lower.CL upper.CL
##     5     76.4  2.26 45.0     71.9     81.0
##    10    119.4  6.50 45.0    106.6    132.8
##    15    172.4 12.80 45.0    147.7    199.1
##    20    235.5 20.80 45.0    195.5    279.2
## 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95 
## Intervals are back-transformed from the sqrt scale 
## Bias adjustment applied based on sigma = (various values)</code></pre>
<p>This example illustrates that it is possible to deal with random
slopes in bias corrections. However it does require some fairly careful
attention to technical details and familiarity with matrix calculations;
so if you don’t have a comfort level with those, it is best to get
outside help.</p>
<div id="adding-variables-not-in-fixed-model-addl.vars" class="section level4">
<h4>Adding variables not in fixed model {addl.vars}</h4>
<!-- @index `addl.vars`; Random predictors!Accessing levels -->
<p>Consider a model like</p>
<pre><code>mod &lt;- lmer(log(response) ~ treatment + (1 + x | subject), data = mydata)</code></pre>
<p>Ordinarily, the reference grid will not include the variable
<code>x</code> because it is not part of the fixed-effects formula.
However, you can include it via the <code>addl.vars</code> argument:</p>
<pre><code>EMM &lt;- emmeans(mod, ~ x|treatment, addl.vars = &quot;x&quot;, at = list(x = -1:1))</code></pre>
<p>We will then obtain EMMs for combinations of <code>treatment</code>
and <code>x</code>. (For a given <code>treatment</code>, all those means
will be equal for every <code>x</code>.) But the bias adjustments
<em>will</em> depend on <code>x</code>.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
</div>
</div>
<div id="offsets" class="section level2">
<h2>Models with offsets</h2>
<!-- @index Offsets!Models with offsets; Examples!Insurance claims (SAS); `ref_grid()`!`offset` -->
<p>If a model is fitted and its formula includes an
<code>offset()</code> term, then by default, the offset is computed and
included in the reference grid. To illustrate, consider a hypothetical
dataset on insurance claims (used as an <a href="https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_genmod_sect006.htm">example
in SAS’s documentation</a>). There are classes of cars of varying counts
(<code>n</code>), sizes (<code>size</code>), and age (<code>age</code>),
and we record the number of insurance claims (<code>claims</code>). We
fit a Poisson model to <code>claims</code> as a function of
<code>size</code> and <code>age</code>. An offset of <code>log(n)</code>
is included so that <code>n</code> functions as an “exposure”
variable.</p>
<pre class="r"><code>ins &lt;- data.frame(
    n = c(500, 1200, 100, 400, 500, 300),
    size = factor(rep(1:3,2), labels = c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;)),
    age = factor(rep(1:2, each = 3)),
    claims = c(42, 37, 1, 101, 73, 14))
ins.glm &lt;- glm(claims ~ size + age + offset(log(n)), 
               data = ins, family = &quot;poisson&quot;)</code></pre>
<p>First, let’s look at the reference grid obtained by default:</p>
<pre class="r"><code>ref_grid(ins.glm)</code></pre>
<pre class="ro"><code>## &#39;emmGrid&#39; object with variables:
##     size = S, M, L
##     age = 1, 2
##     n = 500
## Transformation: &quot;log&quot;</code></pre>
<p>Note that <code>n</code> is included in the reference grid and that
its average value of 500 is displayed. But let’s look at the EMMs:</p>
<pre class="r"><code>(EMM &lt;- emmeans(ins.glm, &quot;size&quot;, type = &quot;response&quot;))</code></pre>
<pre class="ro"><code>##  size rate   SE  df asymp.LCL asymp.UCL
##  S    69.3 6.25 Inf     58.03      82.7
##  M    34.6 3.34 Inf     28.67      41.9
##  L    11.9 3.14 Inf      7.07      19.9
## 
## Results are averaged over the levels of: age 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale</code></pre>
<p>We can see more explicitly what is happening by examining the
internal structure of <code>EMM</code>:</p>
<pre class="r"><code>EMM@grid</code></pre>
<pre class="ro"><code>##   size .offset. .wgt.
## 1    S 6.214608     2
## 2    M 6.214608     2
## 3    L 6.214608     2</code></pre>
<p>and note that <span class="math inline">\(\log(500) \approx
6.215\)</span> is used as the offset value in calculating these
estimates.</p>
<p>All this said, many users would like to ignore that average offset
for this kind of model, and instead use one corresponding to
<code>n = 1</code>, because then the estimates we obtain are estimated
rates per unit <code>n</code>. This may be accomplished by specifying an
<code>offset</code> parameter in the call:</p>
<pre class="r"><code>emmeans(ins.glm, &quot;size&quot;, type = &quot;response&quot;, offset = log(1))</code></pre>
<pre class="ro"><code>##  size   rate      SE  df asymp.LCL asymp.UCL
##  S    0.1385 0.01250 Inf    0.1161    0.1653
##  M    0.0693 0.00669 Inf    0.0573    0.0837
##  L    0.0237 0.00627 Inf    0.0141    0.0398
## 
## Results are averaged over the levels of: age 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale</code></pre>
<p>An alternative way to achieve the same results is to set
<code>n</code> equal to 1 in the reference grid itself (output not
shown, because it is identical):</p>
<pre class="r"><code>emmeans(ins.glm, &quot;size&quot;, type = &quot;response&quot;, at = list(n = 1))</code></pre>
<p>By the way, you may set some other reference value for the rates. For
example, if you want estimates of claims per 100 cars, simply use
(results not shown):</p>
<pre class="r"><code>emmeans(ins.glm, &quot;size&quot;, type = &quot;response&quot;, offset = log(100))</code></pre>
<p>For more details on how offsets are handled, and how and why an
<code>offset()</code> <em>model term</em> is treated differently than an
<code>offset</code> <em>argument</em> in model fitting, see <a href="xplanations.html#offsets">the “xplanations” vignette</a>.</p>
<p>An additional complication may come up in models zero-inflated or
hurdle models. In those cases, it is somewhat ambiguous what one might
mean by a “rate”, but one interpretation would be to just go with the
above techniques with estimates for just the Poisson component of the
model. Another approach would be to estimate the response mean with the
zero-inflation included, and divide by the appropriate offset. This can
be done, but it is messy. An example is given on the <a href="https://stats.stackexchange.com/questions/620842/help-interpreting-zeroinfl-results-from-emmeans/621108?noredirect=1#comment1155903_621108">Cross-Validated
site</a>.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
<div id="ordinal" class="section level2">
<h2>Ordinal models</h2>
<!-- @index Ordinal models; Examples!`wine`; Examples!Ordinal model
  Ordinal models!Latent scale -->
<p>Ordinal-response models comprise an example where several options are
available for obtaining EMMs. To illustrate, consider the
<code>wine</code> data in the <strong>ordinal</strong> package. The
response is a rating of bitterness on a five-point scale. we will
consider a probit model in two factors during fermentation:
<code>temp</code> (temperature) and <code>contact</code> (contact with
grape skins), with the judge making the rating as a scale predictor:</p>
<pre class="r"><code>require(&quot;ordinal&quot;)</code></pre>
<pre><code>## Loading required package: ordinal</code></pre>
<pre class="r"><code>wine.clm &lt;- clm(rating ~ temp + contact, scale = ~ judge,
                data = wine, link = &quot;probit&quot;)</code></pre>
<p>(in earlier modeling, we found little interaction between the
factors.) Here are the EMMs for each factor using default options:</p>
<pre class="r"><code>emmeans(wine.clm, list(pairwise ~ temp, pairwise ~ contact))</code></pre>
<pre class="ro"><code>## $`emmeans of temp`
##  temp emmean    SE  df asymp.LCL asymp.UCL
##  cold -0.884 0.290 Inf    -1.452    -0.316
##  warm  0.601 0.225 Inf     0.161     1.041
## 
## Results are averaged over the levels of: contact, judge 
## Confidence level used: 0.95 
## 
## $`pairwise differences of temp`
##  1           estimate    SE  df z.ratio p.value
##  cold - warm    -1.07 0.422 Inf  -2.547  0.0109
## 
## Results are averaged over the levels of: contact, judge 
## 
## $`emmeans of contact`
##  contact emmean    SE  df asymp.LCL asymp.UCL
##  no      -0.614 0.298 Inf   -1.1990   -0.0297
##  yes      0.332 0.201 Inf   -0.0632    0.7264
## 
## Results are averaged over the levels of: temp, judge 
## Confidence level used: 0.95 
## 
## $`pairwise differences of contact`
##  1        estimate    SE  df z.ratio p.value
##  no - yes   -0.684 0.304 Inf  -2.251  0.0244
## 
## Results are averaged over the levels of: temp, judge</code></pre>
<p>These results are on the “latent” scale; the idea is that there is a
continuous random variable (in this case normal, due to the probit link)
having a mean that depends on the predictors; and that the ratings are a
discretization of the latent variable based on a fixed set of cut points
(which are estimated). In this particular example, we also have a scale
model that says that the variance of the latent variable depends on the
judges. The latent results are quite a bit like those for measurement
data, making them easy to interpret. The only catch is that they are not
uniquely defined: we could apply a linear transformation to them, and
the same linear transformation to the cut points, and the results would
be the same.</p>
<div id="ordlp" class="section level6">
<h6></h6>
<!-- @index Ordinal models!Linear-predictor scale -->
<p>The <code>clm</code> function actually fits the model using an
ordinary probit model but with different intercepts for each cut point.
We can get detailed information for this model by specifying
<code>mode = &quot;linear.predictor&quot;</code>:</p>
<pre class="r"><code>tmp &lt;- ref_grid(wine.clm, mode = &quot;lin&quot;)
tmp</code></pre>
<pre class="ro"><code>## &#39;emmGrid&#39; object with variables:
##     temp = cold, warm
##     contact = no, yes
##     judge = 1, 2, 3, 4, 5, 6, 7, 8, 9
##     cut = multivariate response levels: 1|2, 2|3, 3|4, 4|5
## Transformation: &quot;probit&quot;</code></pre>
<p>Note that this reference grid involves an additional constructed
predictor named <code>cut</code> that accounts for the different
intercepts in the model. Let’s obtain EMMs for <code>temp</code> on the
linear-predictor scale:</p>
<pre class="r"><code>emmeans(tmp, &quot;temp&quot;)</code></pre>
<pre class="ro"><code>##  temp emmean    SE  df asymp.LCL asymp.UCL
##  cold  0.884 0.290 Inf     0.316     1.452
##  warm -0.601 0.225 Inf    -1.041    -0.161
## 
## Results are averaged over the levels of: contact, judge, cut 
## Results are given on the probit (not the response) scale. 
## Confidence level used: 0.95</code></pre>
<p>These are just the negatives of the latent results obtained earlier
(the sign is changed to make the comparisons go the right direction).
Closely related to this is <code>mode = &quot;cum.prob&quot;</code> and
<code>mode = &quot;exc.prob&quot;</code>, which simply transform the linear
predictor to cumulative probabilities and exceedance (1 - cumulative)
probabilities. These modes give us access to the details of the fitted
model but are cumbersome to use for describing results. When they can
become useful is when you want to work in terms of a particular cut
point. Let’s look at <code>temp</code> again in terms of the probability
that the rating will be at least 4:</p>
<pre class="r"><code>emmeans(wine.clm, ~ temp, mode = &quot;exc.prob&quot;, at = list(cut = &quot;3|4&quot;))</code></pre>
<pre class="ro"><code>##  temp exc.prob     SE  df asymp.LCL asymp.UCL
##  cold   0.0748 0.0318 Inf    0.0124     0.137
##  warm   0.4069 0.0706 Inf    0.2686     0.545
## 
## Results are averaged over the levels of: contact, judge 
## Confidence level used: 0.95</code></pre>
</div>
<div id="ordprob" class="section level6">
<h6></h6>
<!-- @index Ordinal models!`prob` and `mean.class` -->
<p>There are yet more modes! With <code>mode = &quot;prob&quot;</code>, we obtain
estimates of the probability distribution of each rating. Its reference
grid includes a factor with the same name as the model response – in
this case <code>rating</code>. We usually want to use that as the
primary factor, and the factors of interest as <code>by</code>
variables:</p>
<pre class="r"><code>emmeans(wine.clm, ~ rating | temp, mode = &quot;prob&quot;)</code></pre>
<pre class="ro"><code>## temp = cold:
##  rating   prob     SE  df asymp.LCL asymp.UCL
##  1      0.1292 0.0625 Inf   0.00667    0.2518
##  2      0.4877 0.0705 Inf   0.34948    0.6259
##  3      0.3083 0.0594 Inf   0.19186    0.4248
##  4      0.0577 0.0238 Inf   0.01104    0.1043
##  5      0.0171 0.0127 Inf  -0.00768    0.0419
## 
## temp = warm:
##  rating   prob     SE  df asymp.LCL asymp.UCL
##  1      0.0156 0.0129 Inf  -0.00961    0.0408
##  2      0.1473 0.0448 Inf   0.05959    0.2350
##  3      0.4302 0.0627 Inf   0.30723    0.5532
##  4      0.2685 0.0625 Inf   0.14593    0.3910
##  5      0.1384 0.0506 Inf   0.03923    0.2376
## 
## Results are averaged over the levels of: contact, judge 
## Confidence level used: 0.95</code></pre>
<p>Using <code>mode = &quot;mean.class&quot;</code> obtains the average of these
probability distributions as probabilities of the integers 1–5:</p>
<pre class="r"><code>emmeans(wine.clm, &quot;temp&quot;, mode = &quot;mean.class&quot;)</code></pre>
<pre class="ro"><code>##  temp mean.class    SE  df asymp.LCL asymp.UCL
##  cold       2.35 0.144 Inf      2.06      2.63
##  warm       3.37 0.146 Inf      3.08      3.65
## 
## Results are averaged over the levels of: contact, judge 
## Confidence level used: 0.95</code></pre>
<p>And there is a mode for the scale model too. In this example, the
scale model involves only judges, and that is the only factor in the
grid:</p>
<pre class="r"><code>summary(ref_grid(wine.clm, mode = &quot;scale&quot;), type = &quot;response&quot;)</code></pre>
<pre class="ro"><code>##  judge response    SE  df
##  1        1.000 0.000 Inf
##  2        1.043 0.570 Inf
##  3        1.053 0.481 Inf
##  4        0.710 0.336 Inf
##  5        0.663 0.301 Inf
##  6        0.758 0.341 Inf
##  7        1.071 0.586 Inf
##  8        0.241 0.179 Inf
##  9        0.533 0.311 Inf</code></pre>
<p>Judge 8’s ratings don’t vary much, relative to the others. The scale
model is in terms of log(SD). Again, these are not uniquely
identifiable, and the first level’s estimate is set to log(1) = 0. so,
actually, each estimate shown is a comparison with judge 1.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
</div>
<div id="mcmc" class="section level2">
<h2>Models fitted using MCMC methods</h2>
<!-- @index Bayesian models; Examples!Bayesian model; Examples!`cbpp`
    `rstanarm`; `hpd.summary()`; `summary()`!HPD intervals -->
<p>To illustrate <strong>emmeans</strong>’s support for models fitted
using MCMC methods, consider the <code>example_model</code> available in
the <strong>rstanarm</strong> package. The example concerns CBPP, a
serious disease of cattle in Ethiopia. A generalized linear mixed model
was fitted to the data using the code below. (This is a Bayesian
equivalent of the frequentist model we considered in the <a href="transformations.html#cbpp">“Transformations” vignette</a>.) In
fitting the model, we first set the contrast coding to
<code>bayestestR::contr.bayes</code> because this equalizes the priors
across different treatment levels (a correction from an earlier version
of this vignette.) We subsequently obtain the reference grids for these
models in the usual way. For later use, we also fit the same model with
just the prior information.
<!--- I'm faking this; I actually saved the ref_grid in a system file ---></p>
<pre class="r"><code>cbpp &lt;- transform(lme4::cbpp, unit = 1:56)
require(&quot;bayestestR&quot;)
options(contrasts = c(&quot;contr.bayes&quot;, &quot;contr.poly&quot;))
cbpp.rstan &lt;- rstanarm::stan_glmer(
    cbind(incidence, size - incidence) ~ period + (1|herd) + (1|unit),
    data = cbpp, family = binomial,
    prior = student_t(df = 5, location = 0, scale = 2, autoscale = FALSE),
    chains = 2, cores = 1, seed = 2021.0120, iter = 1000)
cbpp_prior.rstan &lt;- update(cbpp.rstan, prior_PD = TRUE)
cbpp.rg &lt;- ref_grid(cbpp.rstan)
cbpp_prior.rg &lt;- ref_grid(cbpp_prior.rstan)</code></pre>
<!--- here's the system file with the ref_grid --->
<p>Here is the structure of the reference grid:</p>
<pre class="r"><code>cbpp.rg</code></pre>
<pre class="ro"><code>## &#39;emmGrid&#39; object with variables:
##     period = 1, 2, 3, 4
## Transformation: &quot;logit&quot;</code></pre>
<p>So here are the EMMs (no averaging needed in this simple model):</p>
<pre class="r"><code>summary(cbpp.rg)</code></pre>
<pre class="ro"><code>##  period prediction lower.HPD upper.HPD
##  1           -1.60     -2.26    -0.987
##  2           -2.77     -3.65    -1.974
##  3           -2.90     -3.77    -2.040
##  4           -3.32     -4.43    -2.385
## 
## Point estimate displayed: median 
## Results are given on the logit (not the response) scale. 
## HPD interval probability: 0.95</code></pre>
<p>The summary for EMMs of Bayesian models shows the median of the
posterior distribution of each estimate, along with highest posterior
density (HPD) intervals. Under the hood, the posterior sample of
parameter estimates is used to compute a corresponding sample of
posterior EMMs, and it is those that are summarized. (Technical note:
the summary is actually rerouted to the <code>hpd.summary()</code>
function.</p>
<div id="bayesxtra" class="section level6">
<h6></h6>
<!-- @index `as.mcmc()`; **coda** package; **bayesplot** package; 
    **bayestestR** package; Bayes factor; Region of practical equivalence; ROPE -->
<p>We can access the posterior EMMs via the <code>as.mcmc</code> method
for <code>emmGrid</code> objects. This gives us an object of class
<code>mcmc</code> (defined in the <strong>coda</strong> package), which
can be summarized and explored as we please.</p>
<pre class="r"><code>require(&quot;coda&quot;)</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre class="r"><code>summary(as.mcmc(cbpp.rg))</code></pre>
<pre class="ro"><code>## 
## Iterations = 1:500
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 500 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean     SD Naive SE Time-series SE
## period 1 -1.595 0.3333  0.01054        0.01279
## period 2 -2.790 0.4327  0.01368        0.01367
## period 3 -2.916 0.4491  0.01420        0.01706
## period 4 -3.379 0.5384  0.01703        0.01845
## 
## 2. Quantiles for each variable:
## 
##            2.5%    25%    50%    75%   97.5%
## period 1 -2.283 -1.823 -1.597 -1.357 -0.9929
## period 2 -3.707 -3.038 -2.766 -2.511 -2.0197
## period 3 -3.834 -3.196 -2.899 -2.610 -2.0915
## period 4 -4.502 -3.725 -3.318 -3.022 -2.4079</code></pre>
<p>Note that <code>as.mcmc</code> will actually produce an
<code>mcmc.list</code> when there is more than one chain present, as in
this example. The 2.5th and 97.5th quantiles are similar, but not
identical, to the 95% confidence intervals in the frequentist
summary.</p>
<p>The <strong>bayestestR</strong> package provides <code>emmGrid</code>
methods for most of its description and testing functions. For
example:</p>
<pre class="r"><code>bayestestR::bayesfactor_parameters(pairs(cbpp.rg), prior = pairs(cbpp_prior.rg))</code></pre>
<pre><code>## Warning: Bayes factors might not be precise.
##   For precise Bayes factors, sampling at least 40,000 posterior samples is recommended.</code></pre>
<pre class="ro"><code>## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter         |    BF
## -------------------------
## period1 - period2 |  3.01
## period1 - period3 |  5.13
## period1 - period4 | 14.26
## period2 - period3 | 0.173
## period2 - period4 | 0.268
## period3 - period4 | 0.221
## 
## * Evidence Against The Null: 0</code></pre>
<pre class="r"><code>bayestestR::p_rope(pairs(cbpp.rg), range = c(-0.25, 0.25))</code></pre>
<pre class="ro"><code>## Proportion of samples inside the ROPE [-0.25, 0.25]
## 
## Parameter         | p (ROPE)
## ----------------------------
## period1 - period2 |    0.021
## period1 - period3 |    0.015
## period1 - period4 |    0.004
## period2 - period3 |    0.367
## period2 - period4 |    0.184
## period3 - period4 |    0.290</code></pre>
<p>Both of these sets of results suggest that period 1 is different from
the others. For more information on these methods, refer to <a href="https://cran.r-project.org/package=bayestestR">the CRAN page for
<strong>bayestestR</strong></a> and its vignettes, e.g., the one on
Bayes factors.</p>
</div>
<div id="bias-adj-mcmc" class="section level3">
<h3>Bias-adjusted incidence probabilities</h3>
<!-- @index Bias adjustment!in Bayesian models -->
<p>Next, let us consider the back-transformed results. As is discussed
with the <a href="transformations.html#cbpp">frequentist model</a>,
there are random effects present, and if wee want to think in terms of
marginal probabilities across all herds and units, we should correct for
bias; and to do that, we need the standard deviations of the random
effects. The model object has MCMC results for the random effects of
each herd and each unit, but after those, there are also summary results
for the posterior SDs of the two random effects. (I used the
<code>colnames</code> function to find that they are in the 78th and
79th columns.)
<!-- We already read this in earlier, and here's the faked code --></p>
<pre class="r"><code>cbpp.sigma = as.matrix(cbpp.rstan$stanfit)[, 78:79]</code></pre>
<p>Here are the first few:</p>
<pre class="r"><code>head(cbpp.sigma)</code></pre>
<pre class="ro"><code>##           parameters
## iterations Sigma[unit:(Intercept),(Intercept)] Sigma[herd:(Intercept),(Intercept)]
##       [1,]                            1.154694                         0.167807505
##       [2,]                            1.459379                         0.040318460
##       [3,]                            1.482619                         0.006198847
##       [4,]                            1.236694                         0.206057981
##       [5,]                            1.460472                         0.088491844
##       [6,]                            1.412277                         0.070334431</code></pre>
<p>So to obtain bias-adjusted marginal probabilities, obtain the
resultant SD and regrid with bias correction:</p>
<pre class="r"><code>totSD &lt;- sqrt(apply(cbpp.sigma^2, 1, sum))
cbpp.rgrd &lt;- regrid(cbpp.rg, bias.adjust = TRUE, sigma = totSD)
summary(cbpp.rgrd)</code></pre>
<pre class="ro"><code>##  period   prob lower.HPD upper.HPD
##  1      0.2199    0.1324     0.322
##  2      0.0864    0.0329     0.156
##  3      0.0767    0.0241     0.137
##  4      0.0524    0.0120     0.106
## 
## Point estimate displayed: median 
## HPD interval probability: 0.95</code></pre>
<p>Here is a plot of the posterior incidence probabilities,
back-transformed:</p>
<pre class="r"><code>bayesplot::mcmc_areas(as.mcmc(cbpp.rgrd))</code></pre>
<p><img role="img" aria-label="kernel denity estimates for each of the 4 periods. Their medians and spreads decrease with period, and period 1 is especially different. See the previous summary table for the numerical values of the estimated means" src="data:image/png;base64,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" alt="kernel denity estimates for each of the 4 periods. Their medians and spreads decrease with period, and period 1 is especially different. See the previous summary table for the numerical values of the estimated means" width="432" /></p>
<p>… and here are intervals for each period compared with its
neighbor:</p>
<pre class="r"><code>contrast(cbpp.rgrd, &quot;consec&quot;, reverse = TRUE)</code></pre>
<pre class="ro"><code>##  contrast          estimate lower.HPD upper.HPD
##  period1 - period2  0.13283    0.0280     0.235
##  period2 - period3  0.00918   -0.0635     0.097
##  period3 - period4  0.02331   -0.0427     0.103
## 
## Point estimate displayed: median 
## HPD interval probability: 0.95</code></pre>
<p>The only interval that excludes zero is the one that compares periods
1 and 2.</p>
</div>
<div id="predict-mcmc" class="section level3">
<h3>Bayesian prediction</h3>
<!-- @index Predictions!Bayesian models; 
            Predictions!Posterior predictive distribution -->
<p>To predict from an MCMC model, just specify the
<code>likelihood</code> argument in <code>as.mcmc</code>. Doing so
causes the function to simulate data from the posterior predictive
distribution. For example, if we want to predict the CBPP incidence in
future herds of 25 cattle, we can do:</p>
<pre class="r"><code>set.seed(2019.0605)
cbpp.preds &lt;- as.mcmc(cbpp.rgrd, likelihood = &quot;binomial&quot;, trials = 25)
bayesplot::mcmc_hist(cbpp.preds, binwidth = 1)</code></pre>
<p><img role="img" aria-label="Histograms of the predictive distributions for each period. The one for period 1 has bins from 0 to 15; the number of bins decreases until period 4 has only bins for 0 through 5." src="data:image/png;base64,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" alt="Histograms of the predictive distributions for each period. The one for period 1 has bins from 0 to 15; the number of bins decreases until period 4 has only bins for 0 through 5." width="432" /></p>
<p><a href="#contents">Back to Contents</a></p>
<p><a href="vignette-topics.html">Index of all vignette topics</a></p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
