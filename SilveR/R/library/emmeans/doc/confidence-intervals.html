<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="emmeans package, Version 1.10.5" />


<title>Confidence intervals and tests in emmeans</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {font-size: 11pt; font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;margin: 30px 50px 30px 50px; }h1,h2,h3,h4,h5,h6 { font-family: Arial,Helvetica,Sans-serif; }a { text-decoration: none; }a:link { color:darkblue; } a:visited { color:darkblue; } a:hover { color:dodgerblue; }a:active { color:dodgerblue; } code {color: #602000;font-family: "Lucida Console", Monaco, monospace; font-size: 90%;}.r { color: darkred; }.ro { color: darkgreen; background-color: #eeeeee; }.re { color: red;}.r code, a code, .ro code, .re code { color: inherit; }.vigindex ul { list-style-type: none; }.vigindex ul li { list-style: none; }.vigindex a code { color: inherit; }.vigindex li code { color: inherit; }</style>




</head>

<body>




<h1 class="title toc-ignore">Confidence intervals and tests in
emmeans</h1>
<h4 class="author">emmeans package, Version 1.10.5</h4>



<!-- @index Vignettes!Confidence intervals and tests -->
<div id="contents" class="section level2">
<h2>Contents</h2>
<p>This vignette describes various ways of summarizing
<code>emmGrid</code> objects.</p>
<ol style="list-style-type: decimal">
<li><a href="#summary"><code>summary()</code>, <code>confint()</code>,
and <code>test()</code></a></li>
<li><a href="#tran">Back-transforming to response scale</a> (See also
the <a href="transformations.html">“transformations” vignette</a>)</li>
<li><a href="#adjust">Multiplicity adjustments</a></li>
<li><a href="#byvars">Using “by” variables</a></li>
<li><a href="#joint">Joint (omnibus) tests</a></li>
<li><a href="#equiv">Testing equivalence, noninferiority,
nonsuperiority</a></li>
<li>Graphics (in <a href="basics.html#plots">“basics” vignette</a>)</li>
</ol>
<p><a href="vignette-topics.html">Index of all vignette topics</a></p>
</div>
<div id="summary" class="section level2">
<h2><code>summary()</code>, <code>confint()</code>, and
<code>test()</code></h2>
<!-- @index `summary()`; `summary()`!`infer`; `confint()`; `test()`; 
    Confidence intervals; `summary()`!Calculated columns; 
    `summary()`!Show sample size; Sample size, displaying;
            Tests!One- and two-sided; Tests!Nonzero null;
            `summary()`!Bayesian models; `summary()`!`hpd.summary()`;
            Examples!`pigs` -->
<p>The most important method for <code>emmGrid</code> objects is
<code>summary()</code>. For one thing, it is called by default when you
display an <code>emmeans()</code> result. The <code>summary()</code>
function has a lot of options, and the detailed documentation via
<code>help(&quot;summary.emmGrid&quot;)</code> is worth a look.</p>
<p>For ongoing illustrations, let’s re-create some of the objects in the
<a href="basics.html">“basics” vignette</a> for the <code>pigs</code>
example:</p>
<pre class="r"><code>mod4 &lt;- lm(inverse(conc) ~ source + factor(percent), data = pigs)
RG &lt;- ref_grid(mod4)
EMM.source &lt;- emmeans(RG, &quot;source&quot;)</code></pre>
<p>Just <code>summary(&lt;object&gt;)</code> by itself will produce a
summary that varies somewhat according to context. It does this by
setting different defaults for the <code>infer</code> argument, which
consists of two logical values, specifying confidence intervals and
tests, respectively. [The exception is models fitted using MCMC methods,
where <code>summary()</code> is diverted to the
<code>hpd.summary()</code> function, a preferable summary for many
Bayesians.]</p>
<p>The summary of a newly made reference grid will show just estimates
and standard errors, but not confidence intervals or tests (that is,
<code>infer = c(FALSE, FALSE)</code>). The summary of an
<code>emmeans()</code> result, as we see above, will have intervals, but
no tests (i.e., <code>infer = c(TRUE, FALSE)</code>); and the result of
a <code>contrast()</code> call (see <a href="comparisons.html">comparisons and contrasts</a>) will show test
statistics and <em>P</em> values, but not intervals (i.e.,
<code>infer = c(FALSE, TRUE)</code>). There are courtesy methods
<code>confint()</code> and <code>test()</code> that just call
<code>summary()</code> with the appropriate <code>infer</code> setting;
for example,</p>
<pre class="r"><code>test(EMM.source)</code></pre>
<pre class="ro"><code>##  source emmean       SE df t.ratio p.value
##  fish   0.0337 0.000926 23  36.380  &lt;.0001
##  soy    0.0257 0.000945 23  27.141  &lt;.0001
##  skim   0.0229 0.000994 23  22.989  &lt;.0001
## 
## Results are averaged over the levels of: percent 
## Results are given on the inverse (not the response) scale.</code></pre>
<p>It is not particularly useful, though, to test these EMMs against the
default of zero – which is why tests are not usually shown. It makes a
lot more sense to test them against some target concentration, say 40.
And suppose we want to do a one-sided test to see if the concentration
is greater than 40. Remembering that the response is inverse-transformed
in this model, and that the inverse transformation reverses the
direction of comparisons, so that a <em>right</em>-tailed test on the
<code>conc</code> scale becomes a <em>left</em>-tailed test on the
<code>inverse(conc)</code> scale,</p>
<pre class="r"><code>test(EMM.source, null = inverse(40), side = &quot;&lt;&quot;)</code></pre>
<pre class="ro"><code>##  source emmean       SE df  null t.ratio p.value
##  fish   0.0337 0.000926 23 0.025   9.383  1.0000
##  soy    0.0257 0.000945 23 0.025   0.697  0.7535
##  skim   0.0229 0.000994 23 0.025  -2.156  0.0209
## 
## Results are averaged over the levels of: percent 
## Results are given on the inverse (not the response) scale. 
## P values are left-tailed</code></pre>
<p>It is also possible to add calculated columns to the summary, via the
<code>calc</code> argument. The calculations can include any columns up
through <code>df</code> in the summary, as well as any variable in the
object’s <code>grid</code> slot. Among the latter are usually weights in
a column named <code>.wgt.</code>, and we can use that to include sample
size in the summary:</p>
<pre class="r"><code>confint(EMM.source, calc = c(n = ~.wgt.))</code></pre>
<pre class="ro"><code>##  source emmean       SE df  n lower.CL upper.CL
##  fish   0.0337 0.000926 23 10   0.0318   0.0356
##  soy    0.0257 0.000945 23 10   0.0237   0.0276
##  skim   0.0229 0.000994 23  9   0.0208   0.0249
## 
## Results are averaged over the levels of: percent 
## Results are given on the inverse (not the response) scale. 
## Confidence level used: 0.95</code></pre>
<p><a href="#contents">Back to Contents</a></p>
</div>
<div id="tran" class="section level2">
<h2>Back-transforming</h2>
<!-- @index Transformations!Back-transforming; `type`; Response scale -->
<p>Transformations and link functions are supported in several ways in
<strong>emmeans</strong>, making this a complex topic worthy of <a href="transformations.html">its own vignette</a>. Here, we show just the
most basic approach. Namely, specifying the argument
<code>type = &quot;response&quot;</code> will cause the displayed results to be
back-transformed to the response scale, when a transformation or link
function is incorporated in the model. For example, let’s try the
preceding <code>test()</code> call again:</p>
<pre class="r"><code>test(EMM.source, null = inverse(40), side = &quot;&lt;&quot;, type = &quot;response&quot;)</code></pre>
<pre class="ro"><code>##  source response    SE df null t.ratio p.value
##  fish       29.7 0.816 23   40   9.383  1.0000
##  soy        39.0 1.440 23   40   0.697  0.7535
##  skim       43.8 1.900 23   40  -2.156  0.0209
## 
## Results are averaged over the levels of: percent 
## P values are left-tailed 
## Tests are performed on the inverse scale</code></pre>
<p>Note what changes and what doesn’t change. In the <code>test()</code>
call, we <em>still</em> use the 1/40 as the null value;
<code>null</code> must always be specified on the linear-prediction
scale, in this case the inverse. In the output, the displayed estimates,
as well as the <code>null</code> value, are shown back-transformed. As
well, the standard errors are altered (using the delta method). However,
the <em>t</em> ratios and <em>P</em> values are identical to the
preceding results. That is, the tests themselves are still conducted on
the linear-predictor scale (as is noted in the output).</p>
<p>Similar statements apply to confidence intervals on the response
scale:</p>
<pre class="r"><code>confint(EMM.source, side = &quot;&lt;&quot;, level = .90, type = &quot;response&quot;)</code></pre>
<pre class="ro"><code>##  source response    SE df lower.CL upper.CL
##  fish       29.7 0.816 23     28.6      Inf
##  soy        39.0 1.440 23     37.2      Inf
##  skim       43.8 1.900 23     41.4      Inf
## 
## Results are averaged over the levels of: percent 
## Confidence level used: 0.9 
## Intervals are back-transformed from the inverse scale</code></pre>
<p>With <code>side = &quot;&lt;&quot;</code>, an <em>upper</em> confidence limit
is computed on the inverse scale, then that limit is back-transformed to
the response scale; and since <code>inverse</code> reverses everything,
those upper confidence limits become lower ones on the response scale.
(We have also illustrated how to change the confidence level.)</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
<div id="adjust" class="section level2">
<h2>Multiplicity adjustments</h2>
<!-- @index Multiplicity adjustments; `adjust`; 
     Tukey adjustment; Sidak adjustment -->
<p>Both tests and confidence intervals may be adjusted for simultaneous
inference. Such adjustments ensure that the confidence coefficient for a
whole set of intervals is at least the specified level, or to control
for multiplicity in a whole family of tests. This is done via the
<code>adjust</code> argument. For <code>ref_grid()</code> and
<code>emmeans()</code> results, the default is
<code>adjust = &quot;none&quot;</code>. For most <code>contrast()</code> results,
<code>adjust</code> is often something else, depending on what type of
contrasts are created. For example, pairwise comparisons default to
<code>adjust = &quot;tukey&quot;</code>, i.e., the Tukey HSD method. The
<code>summary()</code> function sometimes <em>changes</em>
<code>adjust</code> if it is inappropriate. For example, with</p>
<pre class="r"><code>confint(EMM.source, adjust = &quot;tukey&quot;)</code></pre>
<pre><code>## Note: adjust = &quot;tukey&quot; was changed to &quot;sidak&quot;
## because &quot;tukey&quot; is only appropriate for one set of pairwise comparisons</code></pre>
<pre class="ro"><code>##  source emmean       SE df lower.CL upper.CL
##  fish   0.0337 0.000926 23   0.0313   0.0361
##  soy    0.0257 0.000945 23   0.0232   0.0281
##  skim   0.0229 0.000994 23   0.0203   0.0254
## 
## Results are averaged over the levels of: percent 
## Results are given on the inverse (not the response) scale. 
## Confidence level used: 0.95 
## Conf-level adjustment: sidak method for 3 estimates</code></pre>
<p>the adjustment is changed to the Sidak method because the Tukey
adjustment is inappropriate unless you are doing pairwise
comparisons.</p>
<div id="adjmore" class="section level7">
<p class="heading"></p>
<!-- @index **mvtnorm** package; Bonferroni adjustment
     Multivariate *t* (`"mvt"`) adjustment; Unadjusted tests -->
<p>An adjustment method that is usually appropriate is Bonferroni;
however, it can be quite conservative. Using <code>adjust = &quot;mvt&quot;</code>
is the closest to being the “exact” all-around method “single-step”
method, as it uses the multivariate <em>t</em> distribution (and the
<strong>mvtnorm</strong> package) with the same covariance structure as
the estimates to determine the adjustment. However, this comes at high
computational expense as the computations are done using simulation
techniques. For a large set of tests (and especially confidence
intervals), the computational lag becomes noticeable if not
intolerable.</p>
<p>For tests, <code>adjust</code> increases the <em>P</em> values over
those otherwise obtained with <code>adjust = &quot;none&quot;</code>. Compare the
following adjusted tests with the unadjusted ones previously
computed.</p>
<pre class="r"><code>test(EMM.source, null = inverse(40), side = &quot;&lt;&quot;, adjust = &quot;bonferroni&quot;)</code></pre>
<pre class="ro"><code>##  source emmean       SE df  null t.ratio p.value
##  fish   0.0337 0.000926 23 0.025   9.383  1.0000
##  soy    0.0257 0.000945 23 0.025   0.697  1.0000
##  skim   0.0229 0.000994 23 0.025  -2.156  0.0627
## 
## Results are averaged over the levels of: percent 
## Results are given on the inverse (not the response) scale. 
## P value adjustment: bonferroni method for 3 tests 
## P values are left-tailed</code></pre>
<p><a href="#contents">Back to Contents</a></p>
</div>
</div>
<div id="byvars" class="section level2">
<h2>“By” variables</h2>
<!-- @index `by` groups; Grouping into separate sets -->
<p>Sometimes you want to break a summary down into smaller pieces; for
this purpose, the <code>by</code> argument in <code>summary()</code> is
useful. For example,</p>
<pre class="r"><code>confint(RG, by = &quot;source&quot;)</code></pre>
<pre class="ro"><code>## source = fish:
##  percent prediction      SE df lower.CL upper.CL
##        9     0.0385 0.00135 23   0.0357   0.0413
##       12     0.0333 0.00125 23   0.0307   0.0359
##       15     0.0326 0.00138 23   0.0297   0.0354
##       18     0.0304 0.00138 23   0.0275   0.0332
## 
## source = soy:
##  percent prediction      SE df lower.CL upper.CL
##        9     0.0305 0.00126 23   0.0279   0.0331
##       12     0.0253 0.00124 23   0.0227   0.0278
##       15     0.0245 0.00128 23   0.0219   0.0272
##       18     0.0223 0.00162 23   0.0190   0.0257
## 
## source = skim:
##  percent prediction      SE df lower.CL upper.CL
##        9     0.0277 0.00127 23   0.0251   0.0303
##       12     0.0225 0.00125 23   0.0199   0.0250
##       15     0.0217 0.00139 23   0.0189   0.0246
##       18     0.0195 0.00163 23   0.0162   0.0229
## 
## Results are given on the inverse (not the response) scale. 
## Confidence level used: 0.95</code></pre>
<p>If there is also an <code>adjust</code> in force when <code>by</code>
variables are used, by default, the adjustment is made
<em>separately</em> on each <code>by</code> group; e.g., in the above,
we would be adjusting for sets of 4 intervals, not all 12 together (but
see “cross-adjustments” below.)</p>
<p>There can be a <code>by</code> specification in
<code>emmeans()</code> (or equivalently, a <code>|</code> in the
formula); and if so, it is passed on to <code>summary()</code> and used
unless overridden by another <code>by</code>. Here are examples, not
run:</p>
<pre class="r"><code>emmeans(mod4, ~ percent | source)     ### same results as above
summary(.Last.value, by = &quot;percent&quot;)       ### grouped the other way</code></pre>
<p>Specifying <code>by = NULL</code> will remove all grouping.</p>
<div id="cross-adjust" class="section level3">
<h3>Adjustments across <code>by</code> groups</h3>
<!-- @index cross-group comparisons; `cross.adjust`; `adjust`!vs. `cross.adjust`
     `by` groups!Adjusting across groups; Examples!`warpbreaks` -->
<p>As was mentioned, each <code>by</code> group is regarded as a
separate family with regards to the <code>adjust</code> procedure. For
example, consider a model with interaction for the
<code>warpbreaks</code> data, and construct pairwise comparisons of
<code>tension</code> by <code>wool</code>:</p>
<pre class="r"><code>warp.lm &lt;- lm(breaks ~ wool * tension, data = warpbreaks)
warp.pw &lt;- pairs(emmeans(warp.lm, ~ tension | wool))
warp.pw</code></pre>
<pre class="ro"><code>## wool = A:
##  contrast estimate   SE df t.ratio p.value
##  L - M      20.556 5.16 48   3.986  0.0007
##  L - H      20.000 5.16 48   3.878  0.0009
##  M - H      -0.556 5.16 48  -0.108  0.9936
## 
## wool = B:
##  contrast estimate   SE df t.ratio p.value
##  L - M      -0.556 5.16 48  -0.108  0.9936
##  L - H       9.444 5.16 48   1.831  0.1704
##  M - H      10.000 5.16 48   1.939  0.1389
## 
## P value adjustment: tukey method for comparing a family of 3 estimates</code></pre>
<p>We have two sets of 3 comparisons, and the (default) Tukey adjustment
is made <em>separately</em> in each group.</p>
<p>However, sometimes we want the multiplicity adjustment to be broader.
This broadening can be done in two ways. One is to remove the
<code>by</code> variable, which then treats all results as one family.
In our example:</p>
<pre class="r"><code>test(warp.pw, by = NULL, adjust = &quot;bonferroni&quot;)</code></pre>
<pre class="ro"><code>##  contrast wool estimate   SE df t.ratio p.value
##  L - M    A      20.556 5.16 48   3.986  0.0014
##  L - H    A      20.000 5.16 48   3.878  0.0019
##  M - H    A      -0.556 5.16 48  -0.108  1.0000
##  L - M    B      -0.556 5.16 48  -0.108  1.0000
##  L - H    B       9.444 5.16 48   1.831  0.4396
##  M - H    B      10.000 5.16 48   1.939  0.3504
## 
## P value adjustment: bonferroni method for 6 tests</code></pre>
<p>This accomplishes the goal of putting all the comparisons in one
family of 6 comparisons. Note that the Tukey adjustment may not be used
here because we no longer have <em>one</em> set of pairwise
comparisons.</p>
<p>An alternative is to specify <code>cross.adjust</code>, which
specifies an additional adjustment method to apply to corresponding sets
of within-group adjusted <em>P</em> values:</p>
<pre class="r"><code>test(warp.pw, adjust = &quot;tukey&quot;, cross.adjust = &quot;bonferroni&quot;)</code></pre>
<pre class="ro"><code>## wool = A:
##  contrast estimate   SE df t.ratio p.value
##  L - M      20.556 5.16 48   3.986  0.0013
##  L - H      20.000 5.16 48   3.878  0.0018
##  M - H      -0.556 5.16 48  -0.108  1.0000
## 
## wool = B:
##  contrast estimate   SE df t.ratio p.value
##  L - M      -0.556 5.16 48  -0.108  1.0000
##  L - H       9.444 5.16 48   1.831  0.3407
##  M - H      10.000 5.16 48   1.939  0.2777
## 
## P value adjustment: tukey method for comparing a family of 3 estimates 
## Cross-group P-value adjustment: bonferroni</code></pre>
<p>These adjustments are less conservative than the previous result, but
it is still a conservative adjustment to the set of 6 tests. Had we also
specified <code>adjust = &quot;bonferroni&quot;</code>, we would have obtained the
same adjusted <em>P</em> values as we obtained with
<code>by = NULL</code>.</p>
</div>
<div id="simple" class="section level3">
<h3>Simple comparisons</h3>
<!-- @index Simple comparisons; `contrast()`!`simple`; `simple = "each"` -->
<p>There is also a <code>simple</code> argument for
<code>contrast()</code> that is in essence the inverse of
<code>by</code>; the contrasts are run using everything <em>except</em>
the specified variables as <code>by</code> variables. To illustrate,
let’s consider the model for <code>pigs</code> that includes the
interaction (so that the levels of one factor compare differently at
levels of the other factor).</p>
<pre class="r"><code>mod5 &lt;- lm(inverse(conc) ~ source * factor(percent), data = pigs)
RG5 &lt;- ref_grid(mod5)
contrast(RG5, &quot;consec&quot;, simple = &quot;percent&quot;)</code></pre>
<pre class="ro"><code>## source = fish:
##  contrast               estimate      SE df t.ratio p.value
##  percent12 - percent9  -6.64e-03 0.00285 17  -2.328  0.0840
##  percent15 - percent12 -6.68e-05 0.00285 17  -0.023  1.0000
##  percent18 - percent15 -1.40e-03 0.00285 17  -0.489  0.9282
## 
## source = soy:
##  contrast               estimate      SE df t.ratio p.value
##  percent12 - percent9  -4.01e-03 0.00255 17  -1.572  0.3170
##  percent15 - percent12  2.61e-04 0.00255 17   0.102  0.9993
##  percent18 - percent15 -2.18e-03 0.00361 17  -0.605  0.8872
## 
## source = skim:
##  contrast               estimate      SE df t.ratio p.value
##  percent12 - percent9  -5.26e-03 0.00255 17  -2.061  0.1399
##  percent15 - percent12 -2.86e-03 0.00285 17  -1.001  0.6526
##  percent18 - percent15 -3.76e-03 0.00383 17  -0.982  0.6650
## 
## Note: contrasts are still on the inverse scale. Consider using
##       regrid() if you want contrasts of back-transformed estimates. 
## P value adjustment: mvt method for 3 tests</code></pre>
<p>In fact, we may do <em>all</em> one-factor comparisons by specifying
<code>simple = &quot;each&quot;</code>. This typically produces a lot of output,
so use it with care.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
</div>
<div id="joint" class="section level2">
<h2>Joint tests</h2>
<!-- @index `test()`!`joint = TRUE`; `joint`; Type III tests -->
<p>From the above, we already know how to test individual results. For
pairwise comparisons (details in <a href="comparisons.html">the
“comparisons” vignette</a>), we might do</p>
<pre class="r"><code>PRS.source &lt;- pairs(EMM.source)
PRS.source</code></pre>
<pre class="ro"><code>##  contrast    estimate      SE df t.ratio p.value
##  fish - soy   0.00803 0.00134 23   6.009  &lt;.0001
##  fish - skim  0.01083 0.00137 23   7.922  &lt;.0001
##  soy - skim   0.00280 0.00134 23   2.092  0.1136
## 
## Results are averaged over the levels of: percent 
## Note: contrasts are still on the inverse scale. Consider using
##       regrid() if you want contrasts of back-transformed estimates. 
## P value adjustment: tukey method for comparing a family of 3 estimates</code></pre>
<p>But suppose we want an <em>omnibus</em> test that all these
comparisons are zero. Easy enough, using the <code>joint</code> argument
in <code>test</code> (note: the <code>joint</code> argument is
<em>not</em> available in <code>summary()</code>; only in
<code>test()</code>):</p>
<pre class="r"><code>test(PRS.source, joint = TRUE)</code></pre>
<pre class="ro"><code>##  df1 df2 F.ratio p.value note
##    2  23  34.009  &lt;.0001  d  
## 
## d: df1 reduced due to linear dependence</code></pre>
<p>Notice that there are three comparisons, but only 2 d.f. for the
test, as cautioned in the message.</p>
<p>The test produced with <code>joint = TRUE</code> is a “type III” test
(assuming the default equal weights are used to obtain the EMMs). See
more on these types of tests for higher-order effects in the <a href="interactions.html#contrasts">“interactions” vignette section on
contrasts</a>.</p>
<div id="joint_tests" class="section level7">
<p class="heading"></p>
<!-- @index `joint_tests()`; Type III tests; Analysis of variance!Type III -->
<p>For convenience, there is also a <code>joint_tests()</code> function
that performs joint tests of contrasts among each term in a model or
<code>emmGrid</code> object.</p>
<pre class="r"><code>joint_tests(RG5)</code></pre>
<pre class="ro"><code>##  model term     df1 df2 F.ratio p.value
##  source           2  17  30.309  &lt;.0001
##  percent          3  17   8.441  0.0012
##  source:percent   6  17   0.481  0.8135</code></pre>
<p>The tests of main effects are of families of contrasts; those for
interaction effects are for interaction contrasts. These results are
essentially the same as a “Type-III ANOVA”, but may differ in situations
where there are empty cells or other non-estimability issues, or if
generalizations are present such as unequal weighting. (Another
distinction is that sums of squares and mean squares are not shown; that
is because these really are tests of contrasts among predictions, and
they may or may not correspond to model sums of squares.)</p>
<p>One may use <code>by</code> variables with <code>joint_tests</code>.
For example:</p>
<pre class="r"><code>joint_tests(RG5, by = &quot;source&quot;)</code></pre>
<pre class="ro"><code>## source = fish:
##  model term df1 df2 F.ratio p.value
##  percent      3  17   2.967  0.0614
## 
## source = soy:
##  model term df1 df2 F.ratio p.value
##  percent      3  17   1.376  0.2840
## 
## source = skim:
##  model term df1 df2 F.ratio p.value
##  percent      3  17   4.835  0.0130</code></pre>
<p>In some models, it is possible to specify
<code>submodel = &quot;type2&quot;</code>, thereby obtaining something akin to a
Type II analysis of variance. See the <a href="messy-data.html#type2submodel">messy-data vignette</a> for an
example.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
</div>
<div id="equiv" class="section level2">
<h2>Testing equivalence, noninferiority, and nonsuperiority</h2>
<!-- @index Tests!Equivalence; Tests!Noninferiority; `test()`!`delta` -->
<p>The <code>delta</code> argument in <code>summary()</code> or
<code>test()</code> allows the user to specify a threshold value to use
in a test of equivalence, noninferiority, or nonsuperiority. An
equivalence test is kind of a backwards significance test, where small
<em>P</em> values are associated with small differences relative to a
specified threshold value <code>delta</code>. The help page for
<code>summary.emmGrid</code> gives the details of these tests. Suppose
in the present example, we consider two sources to be equivalent if they
are within 0.005 of each other. We can test this as follows:</p>
<pre class="r"><code>test(PRS.source, delta = 0.005, adjust = &quot;none&quot;)</code></pre>
<pre class="ro"><code>##  contrast    estimate      SE df t.ratio p.value
##  fish - soy   0.00803 0.00134 23   2.268  0.9835
##  fish - skim  0.01083 0.00137 23   4.266  0.9999
##  soy - skim   0.00280 0.00134 23  -1.641  0.0572
## 
## Results are averaged over the levels of: percent 
## Note: contrasts are still on the inverse scale. Consider using
##       regrid() if you want contrasts of back-transformed estimates. 
## Statistics are tests of equivalence with a threshold of 0.005 
## P values are left-tailed</code></pre>
<p>Using the 0.005 threshold, the <em>P</em> value is quite small for
comparing soy and skim, providing some statistical evidence that their
difference is enough smaller than the threshold to consider them
equivalent.</p>
<p><a href="#contents">Back to Contents</a></p>
</div>
<div id="graphics" class="section level2">
<h2>Graphics</h2>
<p>Graphical displays of <code>emmGrid</code> objects are described in
the <a href="basics.html#plots">“basics” vignette</a></p>
<p><a href="vignette-topics.html">Index of all vignette topics</a></p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
