<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Satterthwaite</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Satterthwaite</h1>


<div id="TOC">
<ul>
<li><a href="#satterthwaite-degrees-of-freedom-for-asymptotic-covariance" id="toc-satterthwaite-degrees-of-freedom-for-asymptotic-covariance">Satterthwaite
degrees of freedom for asymptotic covariance</a>
<ul>
<li><a href="#one-dimensional-contrast" id="toc-one-dimensional-contrast">One-dimensional contrast</a></li>
<li><a href="#multi-dimensional-contrast" id="toc-multi-dimensional-contrast">Multi-dimensional contrast</a></li>
</ul></li>
<li><a href="#satterthwaite-degrees-of-freedom-for-empirical-covariance" id="toc-satterthwaite-degrees-of-freedom-for-empirical-covariance">Satterthwaite
degrees of freedom for empirical covariance</a>
<ul>
<li><a href="#one-dimensional-contrast-1" id="toc-one-dimensional-contrast-1">One-dimensional contrast</a></li>
<li><a href="#multi-dimensional-contrast-1" id="toc-multi-dimensional-contrast-1">Multi-dimensional
contrast</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>Here we describe the details of the Satterthwaite degrees of freedom
calculations.</p>
<div id="satterthwaite-degrees-of-freedom-for-asymptotic-covariance" class="section level2">
<h2>Satterthwaite degrees of freedom for asymptotic covariance</h2>
<p>In <span class="citation">Christensen (2018)</span> the Satterthwaite
degrees of freedom approximation based on normal models is well detailed
and the computational approach for models fitted with the
<code>lme4</code> package is explained. We follow the algorithm and
explain the implementation in this <code>mmrm</code> package. The model
definition is the same as in <a href="algorithm.html">Details of the
model fitting in <code>mmrm</code></a>.</p>
<p>We are also using the same notation as in the <a href="kenward.html">Details of the Kenward-Roger calculations</a>. In
particular, we assume we have a contrast matrix <span class="math inline">\(C \in \mathbb{R}^{c\times p}\)</span> with which
we want to test the linear hypothesis <span class="math inline">\(C\beta
= 0\)</span>. Further, <span class="math inline">\(W(\hat\theta)\)</span> is the inverse of the
Hessian matrix of the log-likelihood function of <span class="math inline">\(\theta\)</span> evaluated at the estimate <span class="math inline">\(\hat\theta\)</span>, i.e. the observed Fisher
Information matrix as a consistent estimator of the variance-covariance
matrix of <span class="math inline">\(\hat\theta\)</span>. <span class="math inline">\(\Phi(\theta) = \left\{X^\top \Omega(\theta)^{-1}
X\right\} ^{-1}\)</span> is the asymptotic covariance matrix of <span class="math inline">\(\hat\beta\)</span>.</p>
<div id="one-dimensional-contrast" class="section level3">
<h3>One-dimensional contrast</h3>
<p>We start with the case of a one-dimensional contrast, i.e. <span class="math inline">\(c = 1\)</span>. The Satterthwaite adjusted degrees
of freedom for the corresponding t-test are then defined as: <span class="math display">\[
\hat\nu(\hat\theta) = \frac{2f(\hat\theta)^2}{f{&#39;}(\hat\theta)^\top
W(\hat\theta) f{&#39;}(\hat\theta)}
\]</span> where <span class="math inline">\(f(\hat\theta) = C
\Phi(\hat\theta) C^\top\)</span> is the scalar in the numerator and we
can identify it as the variance estimate for the estimated scalar
contrast <span class="math inline">\(C\hat\beta\)</span>. The
computational challenge is essentially to evaluate the denominator in
the expression for <span class="math inline">\(\hat\nu(\hat\theta)\)</span>, which amounts to
computing the <span class="math inline">\(k\)</span>-dimensional
gradient <span class="math inline">\(f{&#39;}(\hat\theta)\)</span> of
<span class="math inline">\(f(\theta)\)</span> (for the given contrast
matrix <span class="math inline">\(C\)</span>) at the estimate <span class="math inline">\(\hat\theta\)</span>. We already have the
variance-covariance matrix <span class="math inline">\(W(\hat\theta)\)</span> of the variance parameter
vector <span class="math inline">\(\theta\)</span> from the model
fitting.</p>
<div id="jacobian-approach" class="section level4">
<h4>Jacobian approach</h4>
<p>However, if we proceeded in a naive way here, we would need to
recompute the denominator again for every chosen <span class="math inline">\(C\)</span>. This would be slow, e.g. when changing
<span class="math inline">\(C\)</span> every time we want to test a
single coefficient within <span class="math inline">\(\beta\)</span>. It
is better to instead evaluate the gradient of the matrix valued function
<span class="math inline">\(\Phi(\theta)\)</span>, which is therefore
the Jacobian, with regards to <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mathcal{J}(\theta) = \nabla_\theta
\Phi(\theta)\)</span>. Imagine <span class="math inline">\(\mathcal{J}(\theta)\)</span> as the the
3-dimensional array with <span class="math inline">\(k\)</span> faces of
size <span class="math inline">\(p\times p\)</span>. Left and right
multiplying each face by <span class="math inline">\(C\)</span> and
<span class="math inline">\(C^\top\)</span> respectively leads to the
<span class="math inline">\(k\)</span>-dimensional gradient <span class="math inline">\(f&#39;(\theta) = C \mathcal{J}(\theta)
C^\top\)</span>. Therefore for each new contrast <span class="math inline">\(C\)</span> we just need to perform simple matrix
multiplications, which is fast (see <code>h_gradient()</code> where this
is implemented). Thus, having computed the estimated Jacobian <span class="math inline">\(\mathcal{J}(\hat\theta)\)</span>, it is only a
matter of putting the different quantities together to compute the
estimate of the denominator degrees of freedom, <span class="math inline">\(\hat\nu(\hat\theta)\)</span>.</p>
</div>
<div id="jacobian-calculation" class="section level4">
<h4>Jacobian calculation</h4>
<p>Currently, we evaluate the gradient of <span class="math inline">\(\Phi(\theta)\)</span> through function
<code>h_jac_list()</code>. It uses automatic differentiation provided in
<code>TMB</code>.</p>
<p>We first obtain the Jacobian of the inverse of the covariance matrix
of coefficient (<span class="math inline">\(\Phi(\theta)^{-1}\)</span>),
following the <a href="kenward.html#special-considerations-for-mmrm-models">Kenward-Roger
calculations</a>. Please note that we only need <span class="math inline">\(P_h\)</span> matrices.</p>
<p>Then, to obtain the Jacobian of the covariance matrix of coefficient,
following the <a href="kenward.html#derivative-of-the-sigma-1">algorithm</a>, we use
<span class="math inline">\(\Phi(\theta)\)</span> estimated in the fit
to obtain the Jacobian.</p>
<p>The result is a list (of length <span class="math inline">\(k\)</span> where <span class="math inline">\(k\)</span> is the dimension of the variance
parameter <span class="math inline">\(\theta\)</span>) of matrices of
<span class="math inline">\(p \times p\)</span>, where <span class="math inline">\(p\)</span> is the dimension of <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div id="multi-dimensional-contrast" class="section level3">
<h3>Multi-dimensional contrast</h3>
<p>When <span class="math inline">\(c &gt; 1\)</span> we are testing
multiple contrasts at once. Here an F-statistic <span class="math display">\[
F = \frac{1}{c} (C\hat\beta)^\top  (C \Phi(\hat\theta) C^\top)^{-1}
C^\top (C\hat\beta)
\]</span> is calculated, and we are interested in estimating an
appropriate denominator degrees of freedom for <span class="math inline">\(F\)</span>, while assuming <span class="math inline">\(c\)</span> are the numerator degrees of freedom.
Note that only in special cases, such as orthogonal or balanced designs,
the F distribution will be exact under the null hypothesis. In general,
it is an approximation.</p>
<p>The calculations are described in detail in <span class="citation">Christensen (2018)</span>, and we don’t repeat them
here in detail. The implementation is in <code>h_df_md_sat()</code> and
starts with an eigen-decomposition of the asymptotic variance-covariance
matrix of the contrast estimate, i.e. <span class="math inline">\(C
\Phi(\hat\theta) C^\top\)</span>. The F-statistic can be rewritten as a
sum of <span class="math inline">\(t^2\)</span> statistics based on
these eigen-values. The corresponding random variables are independent
(by design because they are derived from the orthogonal eigen-vectors)
and essentially have one degree of freedom each. Hence, each of the
<span class="math inline">\(t\)</span> statistics is treated as above in
the one-dimensional contrast case, i.e. the denominator degree of
freedom is calculated for each of them. Finally, using properties of the
F distribution’s expectation, the denominator degree of freedom for the
whole F statistic is derived.</p>
</div>
</div>
<div id="satterthwaite-degrees-of-freedom-for-empirical-covariance" class="section level2">
<h2>Satterthwaite degrees of freedom for empirical covariance</h2>
<p>In <span class="citation">Bell and McCaffrey (2002)</span> the
Satterthwaite degrees of freedom in combination with a sandwich
covariance matrix estimator are described.</p>
<div id="one-dimensional-contrast-1" class="section level3">
<h3>One-dimensional contrast</h3>
<p>For one-dimensional contrast, following the same notation in <a href="algorithm.html">Details of the model fitting in
<code>mmrm</code></a> and <a href="kenward.html">Details of the
Kenward-Roger calculations</a>, we have the following derivation. For an
estimator of variance with the following term</p>
<p><span class="math display">\[
  v = s c^\top(X^\top X)^{-1}\sum_{i}{X_i^\top A_i \epsilon_i
\epsilon_i^\top A_i X_i} (X^\top X)^{-1} c
\]</span></p>
<p>where <span class="math inline">\(s\)</span> takes the value of <span class="math inline">\(\frac{n}{n-1}\)</span>, <span class="math inline">\(1\)</span> or <span class="math inline">\(\frac{n-1}{n}\)</span>, and <span class="math inline">\(A_i\)</span> takes <span class="math inline">\(I_i\)</span>, <span class="math inline">\((I_i -
H_{ii})^{-\frac{1}{2}}\)</span>, or <span class="math inline">\((I_i -
H_{ii})^{-1}\)</span> respectively, <span class="math inline">\(c\)</span> is a column vector, then <span class="math inline">\(v\)</span> can be decomposed into the a weighted
sum of independent <span class="math inline">\(\chi_1^2\)</span>
distribution, where the weights are the eigenvalues of the <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(G\)</span> with elements <span class="math display">\[
  G_{ij} = g_i^\top V g_j
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  g_i = s^{\frac{1}{2}} (I - H)_i^\top A_i X_i (X^\top X)^{-1} c
\]</span> <span class="math display">\[
  H = X(X^\top X)^{-1}X^\top
\]</span></p>
<p><span class="math inline">\((I - H)_i\)</span> corresponds to the
rows of subject <span class="math inline">\(i\)</span>.</p>
<p>So the degrees of freedom can be represented as <span class="math display">\[
  \nu = \frac{(\sum_{i}\lambda_i)^2}{\sum_{i}{\lambda_i^2}}
\]</span></p>
<p>where <span class="math inline">\(\lambda_i, i = 1, \dotsc,
n\)</span> are the eigenvalues of <span class="math inline">\(G\)</span>. <span class="citation">Bell and
McCaffrey (2002)</span> also suggests that <span class="math inline">\(V\)</span> can be chosen as identify matrix, so
<span class="math inline">\(G_{ij} = g_i ^\top g_j\)</span>.</p>
<p>Following <a href="algorithm.html#weighted-least-squares-estimator">Weighted Least
Square Estimator</a>, we can transform the original <span class="math inline">\(X\)</span> into <span class="math inline">\(\tilde{x}\)</span> to use the above equations.</p>
<p>To avoid repeated computation of matrix <span class="math inline">\(A_i\)</span>, <span class="math inline">\(H\)</span> etc for different contrasts, we
calculate and cache the following</p>
<p><span class="math display">\[
  G^\ast_i = (I - H)_i^\top A_i X_i (X^\top X)^{-1}
\]</span> which is a <span class="math inline">\(\sum_i{m_i} \times
p\)</span> matrix. With different contrasts, we need only calculate the
following <span class="math display">\[
  g_i = G^\ast_i c
\]</span> to obtain a <span class="math inline">\(\sum_i{m_i} \times
1\)</span> matrix, <span class="math inline">\(G\)</span> can be
computed with <span class="math inline">\(g_i\)</span>.</p>
<p>To obtain the degrees of freedom, and to avoid eigen computation on a
large matrix, we can use the following equation</p>
<p><span class="math display">\[
  \nu = \frac{(\sum_{i}\lambda_i)^2}{\sum_{i}{\lambda_i^2}} =
\frac{tr(G)^2}{\sum_{i}{\sum_{j}{G_{ij}^2}}}
\]</span></p>
<p>The scale parameter is not used throughout the package.</p>
<p>The proof is as following</p>
<ol style="list-style-type: decimal">
<li>Proof of <span class="math display">\[
  tr(AB) = tr(BA)
\]</span></li>
</ol>
<p>Let <span class="math inline">\(A\)</span> has dimension <span class="math inline">\(p\times q\)</span>, <span class="math inline">\(B\)</span> has dimension <span class="math inline">\(q\times p\)</span> <span class="math display">\[
  tr(AB) = \sum_{i=1}^{p}{(AB)_{ii}} =
\sum_{i=1}^{p}{\sum_{j=1}^{q}{A_{ij}B_{ji}}}
\]</span></p>
<p><span class="math display">\[
  tr(BA) = \sum_{i=1}^{q}{(BA)_{ii}} =
\sum_{i=1}^{q}{\sum_{j=1}^{p}{B_{ij}A_{ji}}}
\]</span></p>
<p>so <span class="math inline">\(tr(AB) = tr(BA)\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Proof of <span class="math display">\[
  tr(G) = \sum_{i}(\lambda_i)
\]</span> and <span class="math display">\[
  \sum_{i}(\lambda_i^2) = \sum_{i}{\sum_{j}{G_{ij}^2}}
\]</span> if <span class="math inline">\(G = G^\top\)</span></li>
</ol>
<p>Following eigen decomposition, we have <span class="math display">\[
  G = Q \Lambda Q^\top
\]</span> where <span class="math inline">\(\Lambda\)</span> is diagonal
matrix, <span class="math inline">\(Q\)</span> is orthogonal matrix.</p>
<p>Using the previous formula that <span class="math inline">\(tr(AB) =
tr(BA)\)</span>, we have</p>
<p><span class="math display">\[
  tr(G) = tr(Q \Lambda Q^\top) = tr(\Lambda Q^\top Q) = tr(\Lambda) =
\sum_{i}(\lambda_i)
\]</span></p>
<p><span class="math display">\[
  tr(G^\top G) = tr(Q \Lambda Q^\top Q \Lambda Q^\top) = tr(\Lambda^2
Q^\top Q) = tr(\Lambda^2) = \sum_{i}(\lambda_i^2)
\]</span></p>
<p>and <span class="math inline">\(tr(G^\top G)\)</span> can be further
expressed as</p>
<p><span class="math display">\[
  tr(G^\top G) = \sum_{i}{(G^\top G)_{ii}} =
\sum_{i}{\sum_{j}{G^\top_{ij}G_{ji}}} = \sum_{i}{\sum_{j}{G_{ij}^2}}
\]</span></p>
</div>
<div id="multi-dimensional-contrast-1" class="section level3">
<h3>Multi-dimensional contrast</h3>
<p>For multi-dimensional contrast we use the same technique for
multi-dimensional contrast for asymptotic covariance.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-bell2002bias" class="csl-entry">
Bell RM, McCaffrey DF (2002). <span>“Bias Reduction in Standard Errors
for Linear Regression with Multi-Stage Samples.”</span> <em>Survey
Methodology</em>, <strong>28</strong>(2), 169–182.
</div>
<div id="ref-Christensen2018" class="csl-entry">
Christensen RHB (2018). <em>Satterthwaite’s Method for Degrees of
Freedom in Linear Mixed Models</em>. Retrieved from <a href="https://github.com/runehaubo/lmerTestR/blob/35dc5885205d709cdc395b369b08ca2b7273cb78/pkg_notes/Satterthwaite_for_LMMs.pdf">https://github.com/runehaubo/lmerTestR/blob/35dc5885205d709cdc395b369b08ca2b7273cb78/pkg_notes/Satterthwaite_for_LMMs.pdf</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
