<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Details of Weighted Least Square Empirical Covariance</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Details of Weighted Least Square Empirical
Covariance</h1>


<div id="TOC">
<ul>
<li><a href="#weighted-least-square-wls-empirical-covariance" id="toc-weighted-least-square-wls-empirical-covariance">Weighted Least
Square (WLS) Empirical Covariance</a></li>
<li><a href="#difference-of-implementations" id="toc-difference-of-implementations">Difference of Implementations</a>
<ul>
<li><a href="#proof-of-identity" id="toc-proof-of-identity">Proof of
Identity</a>
<ul>
<li><a href="#proof-for-covariance-estimator" id="toc-proof-for-covariance-estimator">Proof for Covariance
Estimator</a></li>
<li><a href="#proof-for-degrees-of-freedom" id="toc-proof-for-degrees-of-freedom">Proof for Degrees of
Freedom</a></li>
</ul></li>
</ul></li>
<li><a href="#special-considerations-in-implementations" id="toc-special-considerations-in-implementations">Special
Considerations in Implementations</a>
<ul>
<li><a href="#pseudo-inverse-of-a-matrix" id="toc-pseudo-inverse-of-a-matrix">Pseudo Inverse of a Matrix</a></li>
<li><a href="#avoiding-the-crossproduct-of-the-g-matrix" id="toc-avoiding-the-crossproduct-of-the-g-matrix">Avoiding the
Crossproduct of the G Matrix</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<div id="weighted-least-square-wls-empirical-covariance" class="section level1">
<h1>Weighted Least Square (WLS) Empirical Covariance</h1>
<p>Following the notation we have without weights, <span class="citation">Bell and McCaffrey (2002)</span> and <span class="citation">Pustejovsky and Tipton (2018)</span> suggest</p>
<p><span class="math display">\[
  v = s C^\top(X^\top W X)^{-1}\sum_{i}{X_i^\top W_i A_i \epsilon_i
\epsilon_i^\top A_i W_i X_i} (X^\top W X)^{-1} C
\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> takes <span class="math inline">\(I_i\)</span>, <span class="math inline">\((I_i -
H_{ii})^{-\frac{1}{2}}\)</span>, or <span class="math inline">\((I_i -
H_{ii})^{-1}\)</span> is unchanged, but <span class="math inline">\(H\)</span> is changed</p>
<p><span class="math display">\[
  H = X (X^\top W X)^{-1} X^\top W
\]</span></p>
<p>For the degrees of freedom, we have</p>
<p><span class="math display">\[
  G_{ij} = g_i^\top \Phi g_j
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  g_i = s^{\frac{1}{2}} (I - H)_i^\top A_i W_i X_i (X^\top X)^{-1} C
\]</span></p>
</div>
<div id="difference-of-implementations" class="section level1">
<h1>Difference of Implementations</h1>
<p>Comparing the previous section with our implementation, we can find
out the differences. Since they have nearly the same symbols, to
differentiate the different part, we use subscript <span class="math inline">\(1\)</span> to denote the implementation suggested
by <span class="citation">Bell and McCaffrey (2002)</span> and <span class="citation">Pustejovsky and Tipton (2018)</span>, and use <span class="math inline">\(2\)</span> to denote the our implementation of
covariance estimator in <code>mmrm</code>, we have</p>
<p><span class="math display">\[
  v_{1} = s C^\top(X^\top W X)^{-1}\sum_{i}{X_i^\top W_i A_{1, i}
\epsilon_i \epsilon_i^\top A_{1, i} W_i X_i} (X^\top W X)^{-1} C
\]</span></p>
<p><span class="math display">\[
  v_{2} = s C^\top(X^\top W X)^{-1}\sum_{i}{X_i^\top L_i A_{2, i}
L_i^\top \epsilon_i \epsilon_i^\top L_i A_{2, i} L_i^\top X_i} (X^\top W
X)^{-1} C
\]</span></p>
<p>Here we will prove that they are identical.</p>
<div id="proof-of-identity" class="section level2">
<h2>Proof of Identity</h2>
<div id="proof-for-covariance-estimator" class="section level3">
<h3>Proof for Covariance Estimator</h3>
<p>First of all, we assume that all <span class="math inline">\(A_i\)</span> matrix, in any form, are
positive-definite. Comparing <span class="math inline">\(v_{1}\)</span>
and <span class="math inline">\(v_{2}\)</span>, we see that the
different part is</p>
<p><span class="math display">\[
  M_{1, d, i} = W_i A_{1, i}
\]</span> and <span class="math display">\[
  M_{2, d, i} = L_i A_{2, i} L_i^\top
\]</span></p>
<p>Substitute <span class="math inline">\(H_{1}\)</span> and <span class="math inline">\(H_{2}\)</span> with its expression, we have</p>
<p><span class="math display">\[
  M_{1, d, i} = W_i (I_i - X_i (X^\top W X)^{-1} X_i^\top W_i)^d
\]</span></p>
<p><span class="math display">\[
  M_{2, d, i} = L_i (I_i - L_i^\top X_i (X^\top W X)^{-1} X_i^\top
L_i)^d L_i^\top
\]</span></p>
<p>Where <span class="math inline">\(d\)</span> takes <span class="math inline">\(0\)</span>, <span class="math inline">\(-1/2\)</span> and <span class="math inline">\(-1\)</span> respectively.</p>
<p>Apparently, if <span class="math inline">\(d=0\)</span>, these two
are identical because <span class="math inline">\(W_i = L_i
L_i^\top\)</span>.</p>
<p>When <span class="math inline">\(d = -1\)</span>, we have</p>
<p><span class="math display">\[
  M_{2, -1, i} = L_i (I_i - L_i^\top X_i (X^\top W X)^{-1} X_i^\top
L_i)^{-1} L_i^\top \\
  = (L_i^{-1})^{-1} (I_i - L_i^\top X_i (X^\top W X)^{-1} X_i^\top
L_i)^{-1} ((L_i^\top)^{-1})^{-1} \\
  = [((L_i^\top)^{-1})(I_i - L_i^\top X_i (X^\top W X)^{-1} X_i^\top
L_i)(L_i^{-1})]^{-1} \\
  = [(L_i^\top)^{-1}L_i^{-1} - X_i (X^\top W X)^{-1} X_i^\top]^{-1} \\
  = (W_i^{-1} -  X_i (X^\top W X)^{-1} X_i^\top)^{-1}
\]</span></p>
<p><span class="math display">\[
  M_{1, -1, i} = W_i (I_i - X_i (X^\top W X)^{-1} X_i^\top W_i)^{-1} \\
  = (W_i^{-1})^{-1} (I_i - X_i (X^\top W X)^{-1} X_i^\top W_i)^{-1} \\
  = [(I_i - X_i (X^\top W X)^{-1} X_i^\top W_i)((W_i^{-1}))]^{-1} \\
  = (W_i^{-1} -  X_i (X^\top W X)^{-1} X_i^\top)^{-1}
\]</span></p>
<p>Obviously, <span class="math inline">\(M_{2, -1, i} = M_{1, -1,
i}\)</span>, and use the following notation</p>
<p><span class="math display">\[
  M_{2, -1, i} = L_i B_{2, i} L_i^\top
\]</span></p>
<p><span class="math display">\[
  M_{1, -1, i} = W_i B_{1, i}
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
  B_{1, i} = W_i^{-1} L_i B_{2, i} L_i^\top \\
  = (L_i^\top)^{-1} B_{2, i} L_i^\top
\]</span></p>
<p>When <span class="math inline">\(d = -1/2\)</span>, we have the
following</p>
<p><span class="math display">\[
  M_{2, -1/2, i} = L_i (I_i - L_i^\top X_i (X^\top W X)^{-1} X_i^\top
L_i)^{-1/2} L_i^\top \\
  = L_i B_{2, i}^{1/2} L_i^\top
\]</span></p>
<p><span class="math display">\[
  M_{1, -1/2, i} = W_i (I_i - X_i (X^\top W X)^{-1} X_i^\top W_i)^{-1/2}
\\
  = W_i B_{1, i}^{1/2}
\]</span></p>
<p>Apparently if <span class="math inline">\(B_{1, i}^{1/2} \ne
(L_i^\top)^{-1} B_{2, i}^{1/2} L_i^\top\)</span>, we should also have
<span class="math display">\[
  B_{1, i}^{1/2} B_{1, i}^{1/2} \ne (L_i^\top)^{-1} B_{2, i}^{1/2}
L_i^\top (L_i^\top)^{-1} B_{2, i}^{1/2} L_i^\top
\]</span></p>
<p>leading to</p>
<p><span class="math display">\[
  B_{1, i} \ne (L_i^\top)^{-1} B_{2, i} L_i^\top
\]</span></p>
<p>which is contradictory with our previous result. Thus, these
covariance estimator are identical.</p>
</div>
<div id="proof-for-degrees-of-freedom" class="section level3">
<h3>Proof for Degrees of Freedom</h3>
<p>To prove <span class="math display">\[
  G_{1, ij} = g_{1, i}^\top \Phi g_{1, j}
\]</span> and <span class="math display">\[
  G_{2, ij} = g_{2, i}^\top g_{2, j}
\]</span> are identical, we only need to prove that</p>
<p><span class="math display">\[
  L^{-1} g_{1, i} = g_{mmrm_i}
\]</span></p>
<p>where <span class="math inline">\(\Phi = W^{-1}\)</span> according to
our previous expression.</p>
<p>We first expand <span class="math inline">\(L^{-1} g_{1, i}\)</span>
and <span class="math inline">\(g_{mmrm_i}\)</span></p>
<p><span class="math display">\[
  L^{-1} g_{1, i} = L^{-1} (I - X(X^\top W X)^{-1}X^\top W) S_i^\top
A_{1, i}^d W_i X_i (X^\top W X)^{-1} C
\]</span></p>
<p><span class="math display">\[
  g_{2, i} = (I - L_i^\top X(X^\top W X)^{-1}X^\top L_i) S_i^\top A_{2,
i}^d L_i^\top X_i (X^\top W X)^{-1} C
\]</span></p>
<p>where <span class="math inline">\(S_i\)</span> is the row selection
matrix.</p>
<p>We will prove the inner part equal <span class="math display">\[
  L^{-1} (I - X(X^\top W X)^{-1}X^\top W) S_i^\top A_{1, i}^d W_i = (I -
L^\top X(X^\top W X)^{-1}X^\top L) S_i^\top A_{2, i}^d L_i^\top
\]</span></p>
<p>With the previous proof of covariance estimators, we already have</p>
<p><span class="math display">\[
  M_{1, d, i} = W_i A_{1, i}^d = L_i A_{2, i}^d L_i^\top = M_{2, d, i}
\]</span> we then need to prove <span class="math display">\[
  L^{-1} (I - X(X^\top W X)^{-1}X^\top W) S_i^\top = (I - L^\top
X(X^\top W X)^{-1}X^\top L) S_i^\top L_i^{-1}
\]</span></p>
<p>and note the relationship between <span class="math inline">\((I -
X(X^\top W X)^{-1}X^\top W)\)</span> and <span class="math inline">\((I
- L^\top X(X^\top W X)^{-1}X^\top L)\)</span> has already been proved in
covariance estimator section, we only need to prove</p>
<p><span class="math display">\[
  L^{-1} (I - X(X^\top W X)^{-1}X^\top W) S_i^\top = (I - L^\top
X(X^\top W X)^{-1}X^\top L) S_i^\top L_i^{-1}
\]</span></p>
<p>Apparently</p>
<p><span class="math display">\[
  L^{-1} (I - X(X^\top W X)^{-1}X^\top W) S_i^\top = L^{-1} S_i^\top -
L^{-1} X(X^\top W X)^{-1}X_i^\top W_i
\]</span></p>
<p><span class="math display">\[
  (I - L^\top X(X^\top W X)^{-1}X^\top L) S_i^\top L_i^{-1} = S_i^\top
L_i^{-1} - L^\top X(X^\top W X)^{-1}X_i^\top
\]</span></p>
<p>And obviously <span class="math display">\[
  L^{-1} S_i^\top = S_i^\top L_i^{-1}
\]</span></p>
<p><span class="math display">\[
  L^{-1} X(X^\top W X)^{-1}X_i W_i = L^\top X(X^\top W X)^{-1}X_i^\top
\]</span></p>
<p>because of the following <span class="math display">\[
  (X(X^\top W X)^{-1}X_i W_i)_{i} = X_i(X^\top W X)^{-1}X_i W_i \\
  = W_i X_i(X^\top W X)^{-1}X_i^\top \\
  = (W X(X^\top W X)^{-1}X_i^\top)_{i}
\]</span></p>
</div>
</div>
</div>
<div id="special-considerations-in-implementations" class="section level1">
<h1>Special Considerations in Implementations</h1>
<div id="pseudo-inverse-of-a-matrix" class="section level2">
<h2>Pseudo Inverse of a Matrix</h2>
<p>Empirical covariance matrix is involved with the inverse of a matrix,
or symmetric square root of a matrix. To calculate this, we usually
requires that the matrix is positive-definite. However, <span class="citation">Young (2016)</span> suggest that this is not always
assured in practice.</p>
<p>Thus, following <span class="citation">Pustejovsky and Tipton
(2018)</span>, we use the pseudo inverse to avoid this. We follow the
following logic (see the corresponding <code>C++</code> function
<code>pseudoInverseSqrt</code>) to obtain the pseudo inverse:</p>
<ol style="list-style-type: decimal">
<li>Conduct singular value decomposition.</li>
<li>Use <code>cpow</code> to obtain the square root of the reciprocals
of singular values, if the value is larger than a computational
threshold; otherwise replace the value with 0.</li>
<li>Reconstruct the pseudo inverse matrix from modified singular values
and U/V matrix.</li>
</ol>
<p>In <code>Eigen</code> package, the pseudo inverse method is already
implemented in <a href="https://libeigen.gitlab.io/docs/classEigen_1_1CompleteOrthogonalDecomposition.html#a6260bd1050a28dd59733233d93eb6bed"><code>Eigen::CompleteOrthogonalDecomposition&lt; MatrixType_ &gt;::pseudoInverse</code></a>,
but it is not used for the following reason:</p>
<ol style="list-style-type: decimal">
<li>The pseudo inverse method is not stable and can lead to
<code>NAN</code> in calculations.</li>
<li>To find out the symmetric square root, singular value decomposition
is still needed, so not using the method but instead calculating
directly the square root of the pseudo inverse can be simpler.</li>
</ol>
</div>
<div id="avoiding-the-crossproduct-of-the-g-matrix" class="section level2">
<h2>Avoiding the Crossproduct of the G Matrix</h2>
<p>In the implementation of the empirical covariance matrix with the
Satterthwaite degrees of freedom calculation, it is important to avoid
directly computing the crossproduct of the <span class="math inline">\(G\)</span> matrix (i.e., <span class="math inline">\(G^\top G\)</span>). This is because the <span class="math inline">\(G\)</span> matrix can have a large number of
columns, in situations with many subjects and/or many coefficients,
leading to significant computational and memory overhead. Instead, the
current implementation leverages more efficient matrix operations when
needed using the contrast matrix during the Satterthwaite degrees of
freedom calculation. This approach ensures that the calculation remains
feasible and efficient even for large datasets, without any numerical
differences.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-bell2002bias" class="csl-entry">
Bell RM, McCaffrey DF (2002). <span>“Bias Reduction in Standard Errors
for Linear Regression with Multi-Stage Samples.”</span> <em>Survey
Methodology</em>, <strong>28</strong>(2), 169–182.
</div>
<div id="ref-pustejovsky2018small" class="csl-entry">
Pustejovsky JE, Tipton E (2018). <span>“Small-Sample Methods for
Cluster-Robust Variance Estimation and Hypothesis Testing in Fixed
Effects Models.”</span> <em>Journal of Business &amp; Economic
Statistics</em>, <strong>36</strong>(4), 672–683.
</div>
<div id="ref-young2016improved" class="csl-entry">
Young A (2016). <span>“Improved, Nearly Exact, Statistical Inference
with Robust and Clustered Covariance Matrices Using Effective Degrees of
Freedom Corrections.”</span> <em>Manuscript, London School of
Economics</em>,. Retrieved from <a href="https://personal.lse.ac.uk/YoungA/Improved.pdf">https://personal.lse.ac.uk/YoungA/Improved.pdf</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
