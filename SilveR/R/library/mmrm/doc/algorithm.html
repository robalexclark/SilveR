<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Model Fitting Algorithm</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Model Fitting Algorithm</h1>


<div id="TOC">
<ul>
<li><a href="#model-definition" id="toc-model-definition">Model
definition</a>
<ul>
<li><a href="#linear-model" id="toc-linear-model">Linear model</a></li>
<li><a href="#covariance-matrix-model" id="toc-covariance-matrix-model">Covariance matrix model</a></li>
</ul></li>
<li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation">Maximum Likelihood Estimation</a>
<ul>
<li><a href="#weighted-least-squares-estimator" id="toc-weighted-least-squares-estimator">Weighted least squares
estimator</a></li>
<li><a href="#determinant-and-quadratic-form" id="toc-determinant-and-quadratic-form">Determinant and quadratic
form</a></li>
</ul></li>
<li><a href="#restricted-maximum-likelihood-estimation" id="toc-restricted-maximum-likelihood-estimation">Restricted Maximum
Likelihood Estimation</a>
<ul>
<li><a href="#completing-the-square" id="toc-completing-the-square">Completing the square</a></li>
<li><a href="#objective-function" id="toc-objective-function">Objective
function</a></li>
</ul></li>
</ul>
</div>

<p>Here we describe the exact model definition as well as the estimation
algorithms in detail. After reading through this vignette, you can
follow the implementation of the algorithm in <code>mmrm.cpp</code> and
the covariance structures in <code>covariance.h</code> in the
<code>src</code> directory of this package.</p>
<div id="model-definition" class="section level2">
<h2>Model definition</h2>
<p>The mixed model for repeated measures (MMRM) definition we are using
in this package is the following. Let <span class="math inline">\(i = 1,
\dotsc, n\)</span> denote the subjects from which we observe multiple
observations <span class="math inline">\(j = 1, \dotsc, m_i\)</span>
from total <span class="math inline">\(m_i\)</span> time points <span class="math inline">\(t_{ij} \in \{t_1, \dotsc, t_m\}\)</span>. Note
that the number of time points for a specific subject, <span class="math inline">\(m_i\)</span>, can be smaller than <span class="math inline">\(m\)</span>, when only a subset of the possible
<span class="math inline">\(m\)</span> time points have been
observed.</p>
<div id="linear-model" class="section level3">
<h3>Linear model</h3>
<p>For each subject <span class="math inline">\(i\)</span> we observe a
vector <span class="math display">\[
Y_i = (y_{i1}, \dotsc, y_{im_i})^\top \in \mathbb{R}^{m_i}
\]</span> and given a design matrix <span class="math display">\[
X_i \in \mathbb{R}^{m_i \times p}
\]</span> and a corresponding coefficient vector <span class="math inline">\(\beta \in \mathbb{R}^{p}\)</span> we assume that
the observations are multivariate normal distributed: <span class="math display">\[
Y_i \sim N(X_i\beta, \Sigma_i)
\]</span> where the covariance matrix <span class="math inline">\(\Sigma_i \in \mathbb{R}^{m_i \times m_i}\)</span>
is derived by subsetting the overall covariance matrix <span class="math inline">\(\Sigma \in \mathbb{R}^{m \times m}\)</span>
appropriately by <span class="math display">\[
\Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
\]</span> where the subsetting matrix <span class="math inline">\(S_i
\in \{0, 1\}^{m \times m_i}\)</span> contains in each of its <span class="math inline">\(m_i\)</span> columns contains a single 1
indicating which overall time point is matching <span class="math inline">\(t_{ij}\)</span>. Each row contains at most a
single 1 but can also contain only 0 if this time point was not
observed. For example, assume a subject was observed on time points
<span class="math inline">\(1, 3, 4\)</span> out of total <span class="math inline">\(5\)</span> then the subsetting matrix is <span class="math display">\[
S_i = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\]</span> <span class="math inline">\(G_i \in \mathbb{R}_{\gt 0}^{m_i
\times m_i}\)</span> is the diagonal weight matrix, which is the
identity matrix if no weights are specified. Note that this follows from
the well known property of the multivariate normal distribution that
linear combinations of the random vector again have a multivariate
normal distribution with the correspondingly modified mean vector and
covariance matrix.</p>
<p>Conditional on the design matrices <span class="math inline">\(X_i\)</span>, the coefficient vector <span class="math inline">\(\beta\)</span> and the covariance matrix <span class="math inline">\(\Sigma\)</span> we assume that the observations
are independent between the subjects.</p>
<p>We can write the linear model for all subjects together as <span class="math display">\[
Y = X\beta + \epsilon
\]</span> where <span class="math inline">\(Y \in \mathbb{R}^N\)</span>
combines all subject specific observations vectors <span class="math inline">\(Y_i\)</span> such that we have in total <span class="math inline">\(N = \sum_{i = 1}^{n}{m_i}\)</span> observations,
<span class="math inline">\(X \in \mathbb{R}^{N \times p}\)</span>
combines all subject specific design matrices and <span class="math inline">\(\epsilon \in \mathbb{R}^N\)</span> has a
multivariate normal distribution <span class="math display">\[
\epsilon \sim N(0, \Omega)
\]</span> where <span class="math inline">\(\Omega \in \mathbb{R}^{N
\times N}\)</span> is block-diagonal containing the subject specific
<span class="math inline">\(\Sigma_i\)</span> covariance matrices on the
diagonal and 0 in the remaining entries.</p>
</div>
<div id="covariance-matrix-model" class="section level3">
<h3>Covariance matrix model</h3>
<p>The symmetric and positive definite covariance matrix <span class="math display">\[
\Sigma = \begin{pmatrix}
\sigma_1^2 &amp; \sigma_{12} &amp; \dots &amp; \dots &amp; \sigma_{1m}
\\
\sigma_{21} &amp; \sigma_2^2 &amp; \sigma_{23} &amp; \dots &amp;
\sigma_{2m}\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
\sigma_{m1} &amp; \dots &amp; \dots &amp; \sigma_{m,m-1} &amp;
\sigma_m^2
\end{pmatrix}
\]</span> is parametrized by a vector of variance parameters <span class="math inline">\(\theta = (\theta_1, \dotsc,
\theta_k)^\top\)</span>. There are many different choices for how to
model the covariance matrix and correspondingly <span class="math inline">\(\theta\)</span> has different interpretations.
Since any covariance matrix has a unique Cholesky factorization <span class="math inline">\(\Sigma = LL^\top\)</span> where <span class="math inline">\(L\)</span> is the lower triangular Cholesky
factor, we are going to use this below.</p>
<div id="unstructured-covariance-matrix" class="section level4">
<h4>Unstructured covariance matrix</h4>
<p>The most general model uses a saturated parametrization, i.e. any
covariance matrix could be represented in this form. Here we use <span class="math display">\[
L = D\tilde{L}
\]</span> where <span class="math inline">\(D\)</span> is the diagonal
matrix of standard deviations, and <span class="math inline">\(\tilde{L}\)</span> is a unit diagonal lower
triangular matrix. Hence we start <span class="math inline">\(\theta\)</span> with the natural logarithm of the
standard deviations, followed by the row-wise filled entries of <span class="math inline">\(\tilde{L} = \{l_{ij}\}_{1 \leq j &lt; i \leq
m}\)</span>: <span class="math display">\[
\theta = (
  \log(\sigma_1), \dotsc, \log(\sigma_m),
  l_{21}, l_{31}, l_{32}, \dotsc, l_{m,m-1}
)^\top
\]</span> Here <span class="math inline">\(\theta\)</span> has <span class="math inline">\(k = m(m+1)/2\)</span> entries. For example for
<span class="math inline">\(m = 4\)</span> time points we need <span class="math inline">\(k = 10\)</span> variance parameters to model the
unstructured covariance matrix.</p>
<p>Other covariance matrix choices are explained in the <a href="covariance.html">covariance structures vignette</a>.</p>
</div>
<div id="grouped-covariance-matrix" class="section level4">
<h4>Grouped covariance matrix</h4>
<p>In some cases, we would like to estimate unique covariance matrices
across groups, while keeping the covariance structure (unstructured,
ante-dependence, Toeplitz, etc.) consistent across groups. Following the
notations in the previous section, for subject <span class="math inline">\(i\)</span> in group <span class="math inline">\(g(i)\)</span>, we have</p>
<p><span class="math display">\[
\Sigma_{i} = S_i^\top \Sigma_{g(i)} S_i
\]</span></p>
<p>where <span class="math inline">\(g(i)\)</span> is the group of
subject <span class="math inline">\(i\)</span> and <span class="math inline">\(\Sigma_{g(i)}\)</span> is the covariance matrix of
group <span class="math inline">\(g(i)\)</span>.</p>
<p>The parametrization of <span class="math inline">\(\theta\)</span> is
similar to other non-grouped <span class="math inline">\(\theta\)</span>. Assume that there are total
number of <span class="math inline">\(G\)</span> groups, the length of
<span class="math inline">\(\theta\)</span> is multiplied by <span class="math inline">\(G\)</span>, and for each part, <span class="math inline">\(\theta\)</span> is parametrized in the same
fashion. For example, for an unstructured covariance matrix, <span class="math inline">\(\theta\)</span> has <span class="math inline">\(k
= G * m(m+1)/2\)</span> entries.</p>
</div>
<div id="spatial-covariance-matrix" class="section level4">
<h4>Spatial covariance matrix</h4>
<p>A spatial covariance structure can model individual-specific visit
times. An individual’s covariance matrix is then a function of both the
population-level covariance parameters (specific to the chosen
structure) and the individual’s visit times. Following the notations in
the previous section, for subject <span class="math inline">\(i\)</span>
with total number of <span class="math inline">\(m_i\)</span> visits, we
have</p>
<p><span class="math display">\[
\sigma_{ijk} = \sigma * f(dist(\boldsymbol{c}_{ij},
\boldsymbol{c}_{ik}))
\]</span></p>
<p>The <span class="math inline">\((m_{ij}, m_{ik})\)</span> element of
<span class="math inline">\(\Sigma_{i}\)</span> is a function of the
distance between <span class="math inline">\(m_{ij}\)</span> and <span class="math inline">\(m_{ik}\)</span> visit occurring on <span class="math inline">\(t_{m_{ij}}\)</span> and <span class="math inline">\(t_{m_{ik}}\)</span>. <span class="math inline">\(t_{m_{ij}}\)</span> is the coordinate(time) of
<span class="math inline">\(m_{ij}\)</span> visit for subject <span class="math inline">\(i\)</span>. <span class="math inline">\(\sigma\)</span> is the constant variance. Usually
we use Euclidean distance.</p>
<p>Currently only spatial exponential covariance structure is
implemented. For coordinates with multiple dimensions, the Euclidean
distance is used without transformations.</p>
</div>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2>Maximum Likelihood Estimation</h2>
<p>Given the general linear model above, and conditional on <span class="math inline">\(\theta\)</span>, we know that the likelihood for
<span class="math inline">\(\beta\)</span> is <span class="math display">\[
L(\beta; Y) = (2\pi)^{-N/2} \det(\Omega)^{-1/2}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
\]</span> and we also know that the maximum likelihood (ML) estimate of
<span class="math inline">\(\beta\)</span> is the weighted least squares
estimator <span class="math inline">\(\hat{\beta}\)</span> solving the
estimating equation <span class="math display">\[
(X^\top \Omega^{-1} X) \hat{\beta} = X^\top \Omega^{-1} Y.
\]</span> Plugging in <span class="math inline">\(\hat{\beta}\)</span>
into the likelihood above gives then the value of the function we want
to maximize with regards to the variance parameters <span class="math inline">\(\theta\)</span>. Practically this will be done on
the negative log scale: <span class="math display">\[
f(\theta; \hat{\beta}) = - \log L(\hat{\beta}; Y) = \frac{N}{2}
\log(2\pi) +
  \frac{1}{2}\log\det(\Omega) +
  \frac{1}{2} (Y - X\hat{\beta})^\top \Omega^{-1} (Y - X\hat{\beta})
\]</span> The objective function <span class="math inline">\(f(\theta;
\hat{\beta})\)</span> is then minimized with numerical optimizers
utilizing quasi-Newton-Raphson algorithms based on the gradient (or
additionally with Hessian, see <a href="introduction.html#optimizer">optimizer</a>). Here the use of the
Template Model Builder package <code>TMB</code> is helpful because</p>
<ol style="list-style-type: decimal">
<li><code>TMB</code> allows to perform the calculations in C++, which
maximizes the speed.</li>
<li><code>TMB</code> performs automatic differentiation of the objective
function with regards to the variance parameters <span class="math inline">\(\theta\)</span>, so that gradient and Hessian do
not have to be approximated numerically or coded explicitly.</li>
</ol>
<div id="weighted-least-squares-estimator" class="section level3">
<h3>Weighted least squares estimator</h3>
<p>Let’s have a look at the details of calculating the log likelihood
above, including in particular the weighted least squares (WLS)
estimator <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Starting point is the linear equation above and the observation that
both the left and right hand sides can be decomposed into
subject-specific terms given the block-diagonal structure of <span class="math inline">\(\Omega\)</span> and therefore its inverse, <span class="math inline">\(W = \Omega^{-1}\)</span>: <span class="math display">\[
X^\top \Omega^{-1} X = X^\top W X =  \sum_{i=1}^{n} X_i^\top W_i X_i
\]</span> and similarly <span class="math display">\[
X^\top \Omega^{-1} Y = X^\top W Y =  \sum_{i=1}^{n} X_i^\top W_i Y_i
\]</span> where <span class="math inline">\(W_i = \Sigma_i^{-1}\)</span>
is the weight matrix for subject <span class="math inline">\(i\)</span>,
the inverse of its covariance matrix.</p>
<p>Instead of calculating this inverse explicitly, it is always better
numerically to work with the Cholesky factorization and solve linear
equations instead. Here we calculate the factorization <span class="math inline">\(\Sigma_i = L_i L_i^\top\)</span>. Note that in the
case where <span class="math inline">\(m_i = m\)</span>, i.e. this
subject has all time points observed, then <span class="math inline">\(\Sigma_i = \Sigma\)</span> and we don’t need to
calculate this again because we have already <span class="math inline">\(\Sigma = L L^\top\)</span>, i.e. <span class="math inline">\(L_i = L\)</span>. Unfortunately, if <span class="math inline">\(m_i &lt; m\)</span>, then we need to calculate
this explicitly, as there is no way to update the Cholesky factorization
for a subset operation <span class="math inline">\(\Sigma_i = S_i^\top
\Sigma S_i\)</span> as we have above. Given <span class="math inline">\(L_i\)</span>, we solve <span class="math display">\[
L_i \tilde{X}_i = X_i
\]</span> for <span class="math inline">\(\tilde{X}_i\)</span> with an
efficient forward-solve, and similarly we solve <span class="math display">\[
L_i \tilde{Y}_i = Y_i
\]</span> for <span class="math inline">\(\tilde{Y}_i\)</span>.
Therefore we have <span class="math display">\[
X_i^\top W_i X_i = \tilde{X}_i^\top \tilde{X}_i
\]</span> and <span class="math display">\[
X_i^\top W_i Y_i = \tilde{X}_i^\top \tilde{Y}_i
\]</span> and we can thereby calculate the left and right hand sides for
the WLS estimating equation. We solve this equation with a robust
Cholesky decomposition with pivoting. The advantage is that we can reuse
this decomposition for calculating the covariance matrix of <span class="math inline">\(\hat{\beta}\)</span>, i.e. <span class="math inline">\(K = (X^\top W X)^{-1}\)</span>, by supplying the
identity matrix as alternative right hand side.</p>
</div>
<div id="determinant-and-quadratic-form" class="section level3">
<h3>Determinant and quadratic form</h3>
<p>For the objective function we also need the log determinant of <span class="math inline">\(\Omega\)</span>: <span class="math display">\[\begin{align}
\frac{1}{2}\log\det(\Omega)
  &amp;= \frac{1}{2}\log\det\{\text{blockdiag} \Sigma_1, \dotsc,
\Sigma_n\} \\
  &amp;= \frac{1}{2}\log\prod_{i=1}^{n}\det{\Sigma_i} \\
  &amp;= \frac{1}{2}\sum_{i=1}^{n}\log\det{L_i L_i^\top} \\
  &amp;= \sum_{i=1}^{n}\log\det{L_i} \\
  &amp;= \sum_{i=1}^{n}\sum_{j=1}^{m_i}\log(l_{i, jj})
\end{align}\]</span> where <span class="math inline">\(l_{i,jj}\)</span>
are the diagonal entries of the factor <span class="math inline">\(L_i\)</span> and we have used that</p>
<ul>
<li>the determinant of a block diagonal matrix is the product of the
determinants of the blocks,</li>
<li>the determinant of the product of matrices is the product of the
determinants,</li>
<li>the determinant of the transposed matrix is the same as the original
one,</li>
<li>the determinant of a triangular matrix is the product of the
diagonal.</li>
</ul>
<p>And finally, for the quadratic form we can reuse the weighted
response vector and design matrix: <span class="math display">\[
(Y - X\hat{\beta})^\top \Omega^{-1} (Y - X\hat{\beta}) =
\sum_{i=1}^{n} (Y_i - X_i\hat{\beta})^\top W_i (Y_i - X_i\hat{\beta}) =
\sum_{i=1}^{n} (\tilde{Y}_i - \tilde{X}_i\hat{\beta})^\top (\tilde{Y}_i
- \tilde{X}_i\hat{\beta})
\]</span></p>
</div>
</div>
<div id="restricted-maximum-likelihood-estimation" class="section level2">
<h2>Restricted Maximum Likelihood Estimation</h2>
<p>Under the restricted ML estimation (REML) paradigm we first obtain
the marginal likelihood of the variance parameters <span class="math inline">\(\theta\)</span> by integrating out the remaining
parameters <span class="math inline">\(\beta\)</span> from the
likelihood. Here we have: <span class="math display">\[
L(\theta; Y) = \int_{\mathbb{R}^p} L(\beta; Y) d\beta =
(2\pi)^{-N/2} \det(\Omega)^{-1/2} \int_{\mathbb{R}^p}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
d\beta
\]</span> where we note that <span class="math inline">\(\det(\Omega)\)</span> depends on <span class="math inline">\(\theta\)</span> but not on <span class="math inline">\(\beta\)</span> and can therefore be pulled out of
the integral.</p>
<div id="completing-the-square" class="section level3">
<h3>Completing the square</h3>
<p>Let’s focus now on the quadratic form in the exponential function and
complete the square with regards to <span class="math inline">\(\beta\)</span> to obtain the kernel of a
multivariate normal distribution: <span class="math display">\[\begin{align}
(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
&amp;= Y^\top \Omega^{-1} Y
+ \beta^\top X^\top \Omega^{-1} X \beta - 2 \beta^\top X^\top
\Omega^{-1} Y \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta
- 2 \beta^\top K^{-1}K X^\top \Omega^{-1} Y \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta - 2 \beta^\top
K^{-1} \hat{\beta} \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta - 2 \beta^\top
K^{-1} \hat{\beta}
+ \hat{\beta}^{-1} K^{-1} \hat{\beta} - \hat{\beta}^{-1} K^{-1}
\hat{\beta} \\
&amp;= Y^\top \Omega^{-1} Y - \hat{\beta}^{-1} K^{-1} \hat{\beta} +
(\beta - \hat{\beta})^\top K^{-1} (\beta - \hat{\beta})
\end{align}\]</span> where we used <span class="math inline">\(K =
(X^\top W X)^{-1}\)</span> and could early on identify <span class="math inline">\(K\)</span> as the covariance matrix of the kernel
of the multivariate normal of <span class="math inline">\(\beta\)</span>
and then later <span class="math inline">\(\hat{\beta}\)</span> as the
mean vector.</p>
<p>With this, we know that the integral of the multivariate normal
kernel is the inverse of the normalizing constants, and thus <span class="math display">\[
\int_{\mathbb{R}^p}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
d\beta =
\exp\left\{
-\frac{1}{2} Y^\top \Omega^{-1} Y + \frac{1}{2} \hat{\beta}^{-1} K^{-1}
\hat{\beta}
\right\}
(2\pi)^{p/2} \det{K}^{1/2}
\]</span> such that the integrated likelihood is <span class="math display">\[
L(\theta; Y) =
(2\pi)^{-(N-p)/2} \det(\Omega)^{-1/2} \det{K}^{1/2}
\exp\left\{
-\frac{1}{2} Y^\top \Omega^{-1} Y + \frac{1}{2} \hat{\beta}^\top K^{-1}
\hat{\beta}
\right\}.
\]</span></p>
</div>
<div id="objective-function" class="section level3">
<h3>Objective function</h3>
<p>As objective function which we want to minimize with regards to the
variance parameters <span class="math inline">\(\theta\)</span> we again
take the negative natural logarithm <span class="math display">\[
f(\theta) = -\log L(\theta;Y) =
\frac{N-p}{2} \log(2\pi) + \frac{1}{2}\log\det(\Omega) -
\frac{1}{2}\log\det(K)
+ \frac{1}{2} \tilde{Y}^\top \tilde{Y} - \frac{1}{2} \hat{\beta}^\top
\tilde{X}^\top \tilde{X} \hat{\beta}
\]</span> It is interesting to see that computation of the REML
objective function is only requiring a few additional calculations
compared to the ML objective function. In particular, since we already
have the matrix decomposition of <span class="math inline">\(K^{-1}\)</span>, it is very easy to obtain the
determinant of it.</p>
<p>Also here we use numeric optimization of <span class="math inline">\(f(\theta)\)</span> and the <code>TMB</code>
library supports this efficiently through automatic differentiation.</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
