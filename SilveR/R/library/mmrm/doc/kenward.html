<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Kenward-Roger</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Kenward-Roger</h1>


<div id="TOC">
<ul>
<li><a href="#model-definition" id="toc-model-definition">Model
definition</a>
<ul>
<li><a href="#linear-model" id="toc-linear-model">Linear model</a></li>
</ul></li>
<li><a href="#mathematical-details-of-kenward-roger-method" id="toc-mathematical-details-of-kenward-roger-method">Mathematical
Details of Kenward-Roger method</a>
<ul>
<li><a href="#special-considerations-for-mmrm-models" id="toc-special-considerations-for-mmrm-models">Special Considerations
for mmrm models</a></li>
<li><a href="#derivative-of-the-overall-covariance-matrix-sigma" id="toc-derivative-of-the-overall-covariance-matrix-sigma">Derivative of
the overall covariance matrix <span class="math inline">\(\Sigma\)</span></a></li>
<li><a href="#derivative-of-the-sigma-1" id="toc-derivative-of-the-sigma-1">Derivative of the <span class="math inline">\(\Sigma^{-1}\)</span></a></li>
<li><a href="#subjects-with-missed-visits" id="toc-subjects-with-missed-visits">Subjects with missed
visits</a></li>
<li><a href="#scenario-under-group-specific-covariance-estimates" id="toc-scenario-under-group-specific-covariance-estimates">Scenario
under group specific covariance estimates</a></li>
<li><a href="#scenario-under-weighted-mmrm" id="toc-scenario-under-weighted-mmrm">Scenario under weighted
mmrm</a></li>
</ul></li>
<li><a href="#inference" id="toc-inference">Inference</a></li>
<li><a href="#parameterization-methods-and-kenward-roger" id="toc-parameterization-methods-and-kenward-roger">Parameterization
methods and Kenward-Roger</a></li>
<li><a href="#implementations-in-mmrm" id="toc-implementations-in-mmrm">Implementations in
<code>mmrm</code></a>
<ul>
<li><a href="#spatial-exponential-derivatives" id="toc-spatial-exponential-derivatives">Spatial Exponential
Derivatives</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>Here we describe the details of the calculations for the
Kenward-Roger degrees of freedom and the adjusted covariance matrix of
the coefficients.</p>
<div id="model-definition" class="section level2">
<h2>Model definition</h2>
<p>The model definition is the same as what we have in <a href="algorithm.html">Details of the model fitting in
<code>mmrm</code></a>. We are using the same notations.</p>
<div id="linear-model" class="section level3">
<h3>Linear model</h3>
<p>For each subject <span class="math inline">\(i\)</span> we observe a
vector <span class="math display">\[
Y_i = (y_{i1}, \dotsc, y_{im_i})^\top \in \mathbb{R}^{m_i}
\]</span> and given a design matrix <span class="math display">\[
X_i \in \mathbb{R}^{m_i \times p}
\]</span> and a corresponding coefficient vector <span class="math inline">\(\beta \in \mathbb{R}^{p}\)</span> we assume that
the observations are multivariate normal distributed: <span class="math display">\[
Y_i \sim N(X_i\beta, \Sigma_i)
\]</span> where the covariance matrix <span class="math inline">\(\Sigma_i \in \mathbb{R}^{m_i \times m_i}\)</span>
is derived by subsetting the overall covariance matrix <span class="math inline">\(\Sigma \in \mathbb{R}^{m \times m}\)</span>
appropriately by <span class="math display">\[
\Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
\]</span> where the subsetting matrix <span class="math inline">\(S_i
\in \{0, 1\}^{m \times m_i}\)</span> contains in each of its <span class="math inline">\(m_i\)</span> columns contains a single 1
indicating which overall time point is matching <span class="math inline">\(t_{ih}\)</span>. <span class="math inline">\(G_i
\in \mathbb{R}_{\gt 0}^{m_i \times m_i}\)</span> is the diagonal weight
matrix.</p>
<p>Conditional on the design matrices <span class="math inline">\(X_i\)</span>, the coefficient vector <span class="math inline">\(\beta\)</span> and the covariance matrix <span class="math inline">\(\Sigma\)</span> we assume that the observations
are independent between the subjects.</p>
<p>We can write the linear model for all subjects together as <span class="math display">\[
Y = X\beta + \epsilon
\]</span> where <span class="math inline">\(Y \in \mathbb{R}^N\)</span>
combines all subject specific observations vectors <span class="math inline">\(Y_i\)</span> such that we have in total <span class="math inline">\(N = \sum_{i = 1}^{n}{m_i}\)</span> observations,
<span class="math inline">\(X \in \mathbb{R}^{N \times p}\)</span>
combines all subject specific design matrices and <span class="math inline">\(\epsilon \in \mathbb{R}^N\)</span> has a
multivariate normal distribution <span class="math display">\[
\epsilon \sim N(0, \Omega)
\]</span> where <span class="math inline">\(\Omega \in \mathbb{R}^{N
\times N}\)</span> is block-diagonal containing the subject specific
<span class="math inline">\(\Sigma_i\)</span> covariance matrices on the
diagonal and 0 in the remaining entries.</p>
</div>
</div>
<div id="mathematical-details-of-kenward-roger-method" class="section level2">
<h2>Mathematical Details of Kenward-Roger method</h2>
<p>The mathematical derivation of the Kenward-Roger method is based on
the Taylor expansion of the obtained covariance matrix of <span class="math inline">\(\hat\beta\)</span> to get a more accurate estimate
for it. All these derivations are based on the restricted maximum
likelihood. Following the same <a href="algorithm.html#covariance-matrix-model">notation</a>, the
covariance matrix, <span class="math inline">\(\Omega\)</span> can be
represented as a function of covariance matrix parameters <span class="math inline">\(\theta = (\theta_1, \dotsc,
\theta_k)^\top\)</span>, i.e. <span class="math inline">\(\Omega(\theta)\)</span>. Here after model fitting
with <code>mmrm</code>, we obtain the estimate <span class="math inline">\(\hat\beta =
\Phi(\hat\theta)X^\top\Omega(\hat\theta)^{-1}Y\)</span>, where <span class="math inline">\(\Phi(\theta) = \left\{X^\top \Omega(\theta)^{-1}
X\right\} ^{-1}\)</span> is the asymptotic covariance matrix of <span class="math inline">\(\hat\beta\)</span>. However, <span class="citation">Kackar and Harville (1984)</span> suggests that
although the <span class="math inline">\(\hat\beta\)</span> is unbiased
for <span class="math inline">\(\beta\)</span>, the covariance matrix,
<span class="math inline">\(\hat\Phi = \left\{X^\top \hat\Omega
X\right\}^{-1}\)</span> can be biased. They showed that the variability
of <span class="math inline">\(\hat\beta\)</span> can be partitioned
into two components,</p>
<p><span class="math display">\[
  \Phi_A = \Phi + \Lambda
\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the
variance-covariance matrix of the asymptotic distribution of <span class="math inline">\(\hat\beta\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> as defined above, and
<span class="math inline">\(\Lambda\)</span> represents the amount to
which the asymptotic variance-covariance matrix underestimates <span class="math inline">\(\Phi_A\)</span>.</p>
<p>Based on a Taylor series expansion around <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\Lambda\)</span> can be approximated by</p>
<p><span class="math display">\[
  \Lambda \simeq \Phi \left\{\sum_{h=1}^k{\sum_{j=1}^k{W_{hj}(Q_{hj} -
P_h \Phi P_j)} }\right\} \Phi
\]</span> where <span class="math display">\[
  P_h = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_h} X
\]</span> <span class="math display">\[
  Q_{hj} = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_h} \Omega
\frac{\partial{\Omega^{-1}}}{\partial \theta_j} X
\]</span></p>
<p><span class="math inline">\(W\)</span> is the inverse of the Hessian
matrix of the log-likelihood function of <span class="math inline">\(\theta\)</span> evaluated at the estimate <span class="math inline">\(\hat\theta\)</span>, i.e. the observed Fisher
Information matrix as a consistent estimator of the variance-covariance
matrix of <span class="math inline">\(\hat\theta\)</span>.</p>
<p>Again, based on a Taylor series expansion about <span class="math inline">\(\theta\)</span>, <span class="citation">Kenward
and Roger (1997)</span> show that <span class="math display">\[
  \hat\Phi \simeq \Phi + \sum_{h=1}^k{(\hat\theta_h -
\theta_h)\frac{\partial{\Phi}}{\partial{\theta_h}}} + \frac{1}{2}
\sum_{h=1}^k{\sum_{j=1}^k{(\hat\theta_h - \theta_h)(\hat\theta_j -
\theta_j)\frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}}}}
\]</span> Ignoring the possible bias in <span class="math inline">\(\hat\theta\)</span>, <span class="math display">\[
  E(\hat\Phi) \simeq \Phi + \frac{1}{2}
\sum_{h=1}^k{\sum_{j=1}^k{W_{hj}\frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}}}}
\]</span> Using previously defined notations, this can be further
written as <span class="math display">\[
  \frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}} = \Phi
(P_h \Phi P_j + P_j \Phi P_h - Q_{hj} - Q_{jh} + R_{hj}) \Phi
\]</span> where <span class="math display">\[
  R_{hj} =
X^\top\Omega^{-1}\frac{\partial^2\Omega}{\partial{\theta_h}\partial{\theta_j}}
\Omega^{-1} X
\]</span></p>
<p>substituting <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\Lambda\)</span> back to the <span class="math inline">\(\hat\Phi_A\)</span>, we have</p>
<p><span class="math display">\[
  \hat\Phi_A = \hat\Phi + 2\hat\Phi
\left\{\sum_{h=1}^k{\sum_{j=1}^k{W_{hj}(Q_{hj} - P_h \hat\Phi P_j -
\frac{1}{4}R_{hj})} }\right\} \hat\Phi
\]</span></p>
<p>where <span class="math inline">\(\Omega(\hat\theta)\)</span>
replaces <span class="math inline">\(\Omega(\theta)\)</span> in the
right-hand side.</p>
<p>Please note that, if we ignore <span class="math inline">\(R_{hj}\)</span>, the second-order derivatives, we
will get a different estimate of adjusted covariance matrix, and we call
this the linear Kenward-Roger approximation.</p>
<div id="special-considerations-for-mmrm-models" class="section level3">
<h3>Special Considerations for mmrm models</h3>
<p>In mmrm models, <span class="math inline">\(\Omega\)</span> is a
block-diagonal matrix, hence we can calculate <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>
and <span class="math inline">\(R\)</span> for each subject and add them
up.</p>
<p><span class="math display">\[
  P_h = \sum_{i=1}^{N}{P_{ih}} = \sum_{i=1}^{N}{X_i^\top
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} X_i}
\]</span></p>
<p><span class="math display">\[
  Q_{hj} = \sum_{i=1}^{N}{Q_{ihj}} = \sum_{i=1}^{N}{X_i^\top
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} \Sigma_i
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
\]</span></p>
<p><span class="math display">\[
  R_{hj} = \sum_{i=1}^{N}{R_{ihj}} =
\sum_{i=1}^{N}{X_i^\top\Sigma_i^{-1}\frac{\partial^2\Sigma_i}{\partial{\theta_h}\partial{\theta_j}}
\Sigma_i^{-1} X_i}
\]</span></p>
</div>
<div id="derivative-of-the-overall-covariance-matrix-sigma" class="section level3">
<h3>Derivative of the overall covariance matrix <span class="math inline">\(\Sigma\)</span></h3>
<p>The derivative of the overall covariance matrix <span class="math inline">\(\Sigma\)</span> with respect to the variance
parameters can be calculated through the derivatives of the Cholesky
factor, and hence obtained through automatic differentiation, following
<a href="https://en.wikipedia.org/wiki/Matrix_calculus#Identities_in_differential_form">matrix
identities calculations</a>. <span class="math display">\[
  \frac{\partial{\Sigma}}{\partial{\theta_h}} =
\frac{\partial{LL^\top}}{\partial{\theta_h}} =
\frac{\partial{L}}{\partial{\theta_h}}L^\top +
L\frac{\partial{L^\top}}{\partial{\theta_h}}
\]</span></p>
<p><span class="math display">\[
  \frac{\partial^2{\Sigma}}{\partial{\theta_h}\partial{\theta_j}} =
\frac{\partial^2{L}}{\partial{\theta_h}\partial{\theta_j}}L^\top +
L\frac{\partial^2{L^\top}}{\partial{\theta_h}\partial{\theta_j}} +
\frac{\partial{L}}{\partial{\theta_h}}\frac{\partial{L^T}}{\partial{\theta_j}}
+
\frac{\partial{L}}{\partial{\theta_j}}\frac{\partial{L^\top}}{\partial{\theta_h}}
\]</span></p>
</div>
<div id="derivative-of-the-sigma-1" class="section level3">
<h3>Derivative of the <span class="math inline">\(\Sigma^{-1}\)</span></h3>
<p>The derivatives of <span class="math inline">\(\Sigma^{-1}\)</span>
can be calculated through</p>
<p><span class="math display">\[
  \frac{\partial{\Sigma\Sigma^{-1}}}{\partial{\theta_h}}\\
  = \frac{\partial{\Sigma}}{\partial{\theta_h}}\Sigma^{-1} +
\Sigma\frac{\partial{\Sigma^{-1}}}{\partial{\theta_h}} \\
  = 0
\]</span> <span class="math display">\[
  \frac{\partial{\Sigma^{-1}}}{\partial{\theta_h}} = - \Sigma^{-1}
\frac{\partial{\Sigma}}{\partial{\theta_h}}\Sigma^{-1}
\]</span></p>
</div>
<div id="subjects-with-missed-visits" class="section level3">
<h3>Subjects with missed visits</h3>
<p>If a subject do not have all visits, the corresponding covariance
matrix can be represented as <span class="math display">\[
  \Sigma_i = S_i^\top \Sigma S_i
\]</span></p>
<p>and the derivatives can be obtained through</p>
<p><span class="math display">\[
  \frac{\partial{\Sigma_i}}{\partial{\theta_h}} = S_i^\top
\frac{\partial{\Sigma}}{\partial{\theta_h}} S_i
\]</span></p>
<p><span class="math display">\[
  \frac{\partial^2{\Sigma_i}}{\partial{\theta_h}\partial{\theta_j}} =
S_i^\top \frac{\partial^2{\Sigma}}{\partial{\theta_h}\partial{\theta_j}}
S_i
\]</span></p>
<p>The derivative of the <span class="math inline">\(\Sigma_i^{-1}\)</span>, <span class="math inline">\(\frac{\partial\Sigma_i^{-1}}{\partial{\theta_h}}\)</span>
can be calculated through <span class="math inline">\(\Sigma_i^{-1}\)</span> and <span class="math inline">\(\frac{\partial{\Sigma_i}}{\partial{\theta_h}}\)</span>
using the above.</p>
</div>
<div id="scenario-under-group-specific-covariance-estimates" class="section level3">
<h3>Scenario under group specific covariance estimates</h3>
<p>When fitting grouped <code>mmrm</code> models, the covariance matrix
for subject i of group <span class="math inline">\(g(i)\)</span>, can be
written as <span class="math display">\[
  \Sigma_i = S_i^\top \Sigma_{g(i)} S_i$.
\]</span> Assume there are <span class="math inline">\(B\)</span>
groups, the number of parameters is increased by <span class="math inline">\(B\)</span> times. With the fact that for each
group, the corresponding <span class="math inline">\(\theta\)</span>
will not affect other parts, we will have block-diagonal <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>
and <span class="math inline">\(R\)</span> matrices, where the blocks
are given by:</p>
<p><span class="math display">\[
P_h = \begin{pmatrix}
P_{h, 1} &amp; \dots &amp; P_{h, B} \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
Q_{hj} = \begin{pmatrix}
Q_{hj, 1} &amp; 0 &amp; \dots &amp; \dots &amp; 0 \\
0 &amp; Q_{hj, 2} &amp; 0 &amp; \dots &amp; 0\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \dots &amp; 0 &amp; Q_{hj, B}
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
R_{hj} = \begin{pmatrix}
R_{hj, 1} &amp; 0 &amp; \dots &amp; \dots &amp; 0 \\
0 &amp; R_{hj, 2} &amp; 0 &amp; \dots &amp; 0\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \dots &amp; 0 &amp; R_{hj, B}
\end{pmatrix}
\]</span></p>
<p>Use <span class="math inline">\(P_{j, b}\)</span> to denote the block
diagonal part for group <span class="math inline">\(b\)</span>, we have
<span class="math display">\[
  P_{h,b} = \sum_{g(i) = b}{P_{ih}} = \sum_{g(i) = b}{X_i^\top
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} X_i}
\]</span></p>
<p><span class="math display">\[
  Q_{hj,b} = \sum_{g(i) = b}{Q_{ihj}} = \sum_{g(i) = b}{X_i^\top
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} \Sigma_i
\frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
\]</span></p>
<p><span class="math display">\[
  R_{hj,b} = \sum_{g(i) = b}{R_{ihj}} = \sum_{g(i) =
b}{X_i^\top\Sigma_i^{-1}\frac{\partial^2\Sigma_i}{\partial{\theta_h}\partial{\theta_j}}
\Sigma_i^{-1} X_i}
\]</span></p>
<p>Similarly for <span class="math inline">\(R\)</span>.</p>
</div>
<div id="scenario-under-weighted-mmrm" class="section level3">
<h3>Scenario under weighted mmrm</h3>
<p>Under weights mmrm model, the covariance matrix for subject <span class="math inline">\(i\)</span>, can be represented as</p>
<p><span class="math display">\[
  \Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
\]</span></p>
<p>Where <span class="math inline">\(G_i\)</span> is a diagonal matrix
of the weights. Then, when deriving <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>
and <span class="math inline">\(R\)</span>, there are no mathematical
differences as they are constant, and having <span class="math inline">\(G_i\)</span> in addition to <span class="math inline">\(S_i\)</span> does not change the algorithms and we
can simply multiply the formulas with <span class="math inline">\(G_i^{-1/2}\)</span>, similarly as above for the
subsetting matrix.</p>
</div>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<p>Suppose we are testing the linear combination of <span class="math inline">\(\beta\)</span>, <span class="math inline">\(C\beta\)</span> with <span class="math inline">\(C
\in \mathbb{R}^{c\times p}\)</span>, we can use the following
F-statistic <span class="math display">\[
  F = \frac{1}{c} (\hat\beta - \beta)^\top  C (C^\top \hat\Phi_A C)^{-1}
C^\top (\hat\beta - \beta)
\]</span> and <span class="math display">\[
  F^* = \lambda F
\]</span> follows exact <span class="math inline">\(F_{c,m}\)</span>
distribution.</p>
<p>When we have only one coefficient to test, then <span class="math inline">\(C\)</span> is a column vector containing a single
<span class="math inline">\(1\)</span> inside. We can still follow the
same calculations as for the multi-dimensional case. This recovers the
degrees of freedom results of the Satterthwaite method.</p>
<p><span class="math inline">\(\lambda\)</span> and <span class="math inline">\(m\)</span> can be calculated through</p>
<p><span class="math display">\[
  M = C (C^\top \Phi C)^{-1} C^\top
\]</span></p>
<p><span class="math display">\[
  A_1 = \sum_{h=1}^k{\sum_{j=1}^k{W_{hj} tr(M \Phi P_h \Phi) tr(M \Phi
P_j \Phi)}}
\]</span></p>
<p><span class="math display">\[
  A_2 = \sum_{h=1}^k{\sum_{j=1}^k{W_{hj} tr(M \Phi P_h \Phi M \Phi P_j
\Phi)}}
\]</span></p>
<p><span class="math display">\[
  B = \frac{1}{2c}(A_1 + 6A_2)
\]</span></p>
<p><span class="math display">\[
  g = \frac{(c+1)A_1 - (c+4)A_2}{(c+2)A_2}
\]</span></p>
<p><span class="math display">\[
  c_1 = \frac{g}{3c+2(1-g)}
\]</span></p>
<p><span class="math display">\[
  c_2 = \frac{c-g}{3c+2(1-g)}
\]</span></p>
<p><span class="math display">\[
  c_3 = \frac{c+2-g}{3c+2(1-g)}
\]</span> <span class="math display">\[E^*={\left\{1-\frac{A_2}{c}\right\}}^{-1}\]</span>
<span class="math display">\[V^*=\frac{2}{c}{\left\{\frac{1+c_1
B}{(1-c_2 B)^2(1-c_3 B)}\right\}}\]</span></p>
<p><span class="math display">\[\rho =
\frac{V^{*}}{2(E^*)^2}\]</span></p>
<p><span class="math display">\[m = 4 + \frac{c+2}{c\rho - 1}\]</span>
<span class="math display">\[\lambda = \frac{m}{E^*(m-2)}\]</span></p>
</div>
<div id="parameterization-methods-and-kenward-roger" class="section level2">
<h2>Parameterization methods and Kenward-Roger</h2>
<p>While the Kenward-Roger adjusted covariance matrix is adopting a
Taylor series to approximate the true value, the choices of
parameterization can change the result. In a simple example of
unstructured covariance structure, in our current approach, where the
parameters are elements of the Cholesky factor of <span class="math inline">\(\Sigma\)</span> (see <a href="covariance.html">parameterization</a>), the second-order
derivatives of <span class="math inline">\(\Sigma\)</span> over our
parameters, are non-zero matrices. However, if we use the elements of
<span class="math inline">\(\Sigma\)</span> as our parameters, then the
second-order derivatives are zero matrices. However, the differences can
be small and will not affect the inference. If you would like to match
SAS results for the unstructured covariance model, you can use the
linear Kenward-Roger approximation.</p>
</div>
<div id="implementations-in-mmrm" class="section level2">
<h2>Implementations in <code>mmrm</code></h2>
<p>In package <code>mmrm</code>, we have implemented Kenward-Roger
calculations based on the previous sections. Specially, for the
first-order and second-order derivatives, we use automatic
differentiation to obtain the results easily for non-spatial covariance
structure. For spatial covariance structure, we derive the exact
results.</p>
<div id="spatial-exponential-derivatives" class="section level3">
<h3>Spatial Exponential Derivatives</h3>
<p>For spatial exponential covariance structure, we have</p>
<p><span class="math display">\[\theta = (\theta_1,\theta_2)\]</span>
<span class="math display">\[\sigma = e^{\theta_1}\]</span> <span class="math display">\[\rho = \frac{e^{\theta_2}}{1 +
e^{\theta_2}}\]</span></p>
<p><span class="math display">\[\Sigma_{ij} = \sigma
\rho^{d_{ij}}\]</span> where <span class="math inline">\(d_{ij}\)</span>
is the distance between time point <span class="math inline">\(i\)</span> and time point <span class="math inline">\(j\)</span>.</p>
<p>So the first-order derivatives can be written as:</p>
<p><span class="math display">\[
  \frac{\partial{\Sigma_{ij}}}{\partial\theta_1} =
\frac{\partial\sigma}{\partial\theta_1} \rho^{d_{ij}}\\
  = e^{\theta_1}\rho^{d_{ij}} \\
  = \Sigma_{ij}
\]</span></p>
<p><span class="math display">\[
  \frac{\partial{\Sigma_{ij}}}{\partial\theta_2} =
\sigma\frac{\partial{\rho^{d_{ij}}}}{\partial\theta_2} \\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\frac{\partial\rho}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\rho(1-\rho) \\
  = \sigma \rho^{d_{ij}} {d_{ij}} (1-\rho)
\]</span></p>
<p>Second-order derivatives can be written as:</p>
<p><span class="math display">\[
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_1\partial\theta_1}\\
  = \frac{\partial\Sigma_{ij}}{\partial\theta_1}\\
  = \Sigma_{ij}
\]</span></p>
<p><span class="math display">\[
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_1\partial\theta_2} =
\frac{\partial^2{\Sigma_{ij}}}{\partial\theta_2\partial\theta_1} \\
  = \frac{\partial\Sigma_{ij}}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\rho(1-\rho)\\
  = \sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)
\]</span></p>
<p><span class="math display">\[
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_2\partial\theta_2}\\
  =
\frac{\partial{\sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)}}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)(d_{ij} (1-\rho) - \rho)
\]</span></p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-kackar1984" class="csl-entry">
Kackar RN, Harville DA (1984). <span>“Approximations for Standard Errors
of Estimators of Fixed and Random Effects in Mixed Linear
Models.”</span> <em>Journal of the American Statistical
Association</em>, <strong>79</strong>(388), 853–862. <a href="https://doi.org/10.1080/01621459.1984.10477102">https://doi.org/10.1080/01621459.1984.10477102.</a>
</div>
<div id="ref-kenward1997" class="csl-entry">
Kenward MG, Roger JH (1997). <span>“Small Sample Inference for Fixed
Effects from Restricted Maximum Likelihood.”</span> <em>Biometrics</em>,
<strong>53</strong>(3), 983–997. <a href="https://doi.org/10.2307/2533558">https://doi.org/10.2307/2533558.</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
